{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models\n",
    "## Day 2 of the summerschool\n",
    "\n",
    "### Summary \n",
    "\n",
    "- We will train the models in pairs $(x,y)$ where $x$ and $y$ will be sequences.\n",
    "- Once the model is trained, given an input sequence $x$ the model will predict\n",
    "  a target sequence $y$.\n",
    "  \n",
    "In order to do so we will implement...\n",
    "  \n",
    "-  one inference algorithm for Hidden Markov Models.\n",
    "    - We will use it to find the most likely hidden state sequence given an observation sequence. \n",
    "    \n",
    "\n",
    "\n",
    "### Notation\n",
    "\n",
    "#### Set of Words $\\Sigma$ and set of states $\\Lambda$\n",
    "This notebook will use the following notation.\n",
    "\n",
    "- $\\Sigma := \\{w_1,\\ldots,w_J\\}$ is the set of words (or vocabulary).\n",
    "- $\\Lambda:= \\{c_1,\\ldots, c_K\\}$ is the set of labels.\n",
    "\n",
    "A sentence is an element of the Kleene clousure of $\\Sigma$, denoted by $\\Sigma^*$.\n",
    "The Kleene clousure of $\\Sigma$, is defined as the set containing all possible sentences of arbitrary lengt that can be created using the words in $\\Sigma$. More formally,\n",
    "\n",
    "$$\n",
    "\\Sigma^* := \\{\\varepsilon\\} \\cup \\Sigma \\cup \\Sigma^2 \\cup \\ldots\n",
    "$$\n",
    "where  $\\{\\varepsilon\\}$ is an \"empty word\". In other words, inputs are observation sequences, $x = x_1 x_2 \\ldots x_N$,  where each $x_i \\in \\Sigma$. \n",
    "\n",
    "Given such an $x$, we seek the corresponding state sequence, $y = y_1 y_2 \\ldots y_N$, \n",
    "where each $y_i \\in \\Lambda$. We also consider two special states: the ${\\tt start}$ symbol,\n",
    "which starts the sequence, and the ${\\tt stop}$ symbol, which ends the sequence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1\n",
    "Consider a person who is only interested in four activities.\n",
    "- walking in the park $({\\tt walk})$,\n",
    "- shopping (${\\tt shop}$),\n",
    "- cleaning the apartment (${\\tt clean}$)\n",
    "- playing tennis (${\\tt tennis}$).\n",
    "\n",
    "Also, consider that the choice of what the person does on a given day is determined exclusively by the weather on that day, which can be either ${\\tt rainy}$ or ${\\tt sunny}$. \n",
    "\n",
    "Now, supposing that we observe what the person did on a sequence of days, the question is: \n",
    "can we use that information to predict the weather on each of those days? \n",
    "\n",
    "To tackle this problem, we assume  that the weather behaves as a discrete Markov chain (with markov property 1): the weather on a given day depends only on the weather on the previous day. The entire system can be described as an HMM.\n",
    "\n",
    "In this example \n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\hline\n",
    "\\Sigma := \\{ {\\tt walk},{\\tt shop},{\\tt clean},{\\tt tennis}\\}\\\\\n",
    "\\Lambda: = \\{ {\\tt rainy},{\\tt sunny} \\} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "Let us assume that we are given access to three different sequences of days, containing both the activities performed by the person and the weather on those days.\n",
    "\n",
    "The information given has the form $(x,y) = (x_i / y_i)$ where $x_i$ is a word in our vocabulary ( ${\\tt walk},{\\tt shop},{\\tt clean},{\\tt tennis}$) and $y_i$ is a state (${\\tt rainy},{\\tt sunny}$). The whole train set is:\n",
    "\n",
    "- (${\\tt walk/rainy, walk/sunny, shop/sunny, clean/sunny}$)\n",
    "- (${\\tt walk/rainy, walk/rainy, shop/rainy, clean/sunny}$)\n",
    "- (${\\tt walk/sunny, shop/sunny, shop/sunny, clean/sunny}$)\n",
    "\n",
    "We will use this information  to train our model.\n",
    "\n",
    "Now assume we are asked to predict the weather conditions on two different\n",
    "sequences of days. During these two sequences, we observed the person performing the following activities: \n",
    "\n",
    "- $({\\tt walk, walk, shop, clean})$\n",
    "- $({\\tt clean, walk, tennis, walk})$\n",
    "\n",
    "\n",
    "The following image represents the first training sequence which starts with  ${\\tt start}$ symbol, and ends with ${\\tt stop}$.\n",
    "\n",
    "<img src=\"./images_for_notebooks/day_2/hmm_new.pdf\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# We will this append to ensure we can import lxmls toolking\n",
    "sys.path.append('../lxmls-toolkit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1, Getting in touch with the provided classes\n",
    "\n",
    "The objective of this exercises is to get in touch with the classes used to store the sequences, you will need this for the next exercise.\n",
    "\n",
    "We will use\n",
    "\n",
    "- class ``Sequence`` in ``lxmls/sequences/sequence.py`` file\n",
    "- class ``LabelDictionary`` in ``lxmls/sequences/label_dictionary.py`` file\n",
    "- class ``SequenceList`` in ``lxmls/sequences/sequence_list.py`` file\n",
    "\n",
    "- class ``_SequenceIterator`` in ``lxmls/sequences/sequence_list.py`` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We could put the code of the classes here with no need to import anything from lxmls-toolkit\n",
    "from lxmls.sequences.label_dictionary import LabelDictionary\n",
    "from lxmls.sequences.sequence import Sequence\n",
    "from lxmls.sequences.sequence_list import SequenceList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will implement the train and test data from example 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleSequence:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Observation set.\n",
    "        self.x_dict = LabelDictionary(['walk', 'shop', 'clean', 'tennis'])\n",
    "        \n",
    "        # State set.\n",
    "        self.y_dict = LabelDictionary(['rainy', 'sunny'])\n",
    "        \n",
    "        # Generate training sequences.\n",
    "        train_sequences = SequenceList(self.x_dict, self.y_dict)\n",
    "        train_sequences.add_sequence(['walk', 'walk', 'shop', 'clean'], ['rainy', 'sunny', 'sunny', 'sunny'])\n",
    "        train_sequences.add_sequence(['walk', 'walk', 'shop', 'clean'], ['rainy', 'rainy', 'rainy', 'sunny'])\n",
    "        train_sequences.add_sequence(['walk', 'shop', 'shop', 'clean'], ['sunny', 'sunny', 'sunny', 'sunny'])\n",
    "\n",
    "        # Generate test sequences.\n",
    "        test_sequences = SequenceList(self.x_dict, self.y_dict)\n",
    "        test_sequences.add_sequence(['walk', 'walk', 'shop', 'clean'], ['rainy', 'sunny', 'sunny', 'sunny'])\n",
    "        test_sequences.add_sequence(['clean', 'walk', 'tennis', 'walk'], ['sunny', 'sunny', 'sunny', 'sunny'])\n",
    "\n",
    "        self.train = train_sequences\n",
    "        self.test = test_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that x_dict and y_dict are ``LabelDictionary``\n",
    "\n",
    "**``LabelDictionary`` objects are instanciated with a list of strings **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data in ``train_sequences`` and ``test_sequences`` are instanciated as ``SequenceList`` objects. \n",
    "\n",
    "**``SequenceList`` objects are instanciated with**\n",
    "\n",
    "- ``x_dict``  containing all possible words $\\Sigma$\n",
    "- ``y_dict``  containing all possible states $\\Lambda$\n",
    "- ``seq_list`` list containing the data (if nothing is passed it starts with an empty list)\n",
    "\n",
    "\n",
    "**``SequenceList`` objects have a method ``add_sequence`` which recieves as input two lists of strings**\n",
    "- ``SequenceList.add_sequence`` appends the given sequence with labels $x,y$ as a ``Sequence`` object.\n",
    "\n",
    "**``Sequence`` objects are instanciated with **:\n",
    "\n",
    "- `` x`` list of observations\n",
    "- `` y`` list of states\n",
    "- `` nr`` length of x and y\n",
    "- ``sequence_list`` \n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we will load the data from Example 2.1 and look at the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk/rainy walk/sunny shop/sunny clean/sunny \n",
      "walk/rainy walk/rainy shop/rainy clean/sunny \n",
      "walk/sunny shop/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "simple = ssr.SimpleSequence()\n",
    "for sequence in simple.train.seq_list: \n",
    "    print sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk/rainy walk/sunny shop/sunny clean/sunny \n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.test.seq_list: \n",
    "    print sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lxmls.sequences.sequence.Sequence"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(simple.train.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nr': 0,\n",
       " 'sequence_list': [walk/rainy walk/sunny shop/sunny clean/sunny , walk/rainy walk/rainy shop/rainy clean/sunny , walk/sunny shop/sunny shop/sunny clean/sunny ],\n",
       " 'x': [0, 0, 1, 2],\n",
       " 'y': [0, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.train.seq_list[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lxmls.sequences.sequence_list.SequenceList"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(simple.train.seq_list[0].sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2]\n",
      "[0, 0, 1, 2]\n",
      "[0, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list:\n",
    "    print sequence.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model details\n",
    "\n",
    "\n",
    "The probability distributions\n",
    "\n",
    "- $P(Y_{i}|Y_{i-1})$ are called transition probabilities; \n",
    "- $P(Y_{1}|Y_{0} = {\\tt start})$ are the initial probabilities\n",
    "- $P(Y_{N+1}={\\tt stop} |Y_{N})$ the final probabilities\n",
    "\n",
    "A first order HMM model has the following independence assumptions over the joint distribution $P(X=x,Y=y)$:\n",
    "\n",
    "- $\\textbf{Independence of previous states.}$: The probability of\n",
    "    being in a given state at position $i$ only depends on\n",
    "    the state of the previous position $i-1$. Formally:\n",
    "    \n",
    "    \\begin{equation*}\n",
    "    P (Y_i = y_i | Y_{i-1} = y_{i-1}, Y_{i-2} = y_{i-2}, \\ldots, Y_1 = y_1) = P (Y_i = y_i | Y_{i-1} = y_{i-1})\n",
    "    \\end{equation*} \n",
    "    \n",
    "    defining a first order Markov chain\n",
    "\n",
    "\n",
    "- $\\textbf{Homogeneous transition.}$: The probability of\n",
    "    making a transition from state $c_l$ to state $c_k$ is independent of\n",
    "    the particular position in the sequence. That is, for all $i,t \\in \\{1,\\ldots,N\\}$,\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P (Y_i = c_k | Y_{i-1} = c_l) =  P (Y_{t} = c_k | Y_{t-1} = c_l)\n",
    "     \\end{equation*}\n",
    "\n",
    "\n",
    "- $\\textbf{Observation independence.}$  The probability of\n",
    "    observing $X_i = x_i$ at position $i$ is fully determined by the state $Y_i$\n",
    "    at that position. Formally, \n",
    "    \n",
    "     \\begin{equation*}\n",
    "     P (X_i = x_i | Y_1=y_1, \\ldots, Y_i=y_i, \\ldots, Y_N=y_N) = P(X_i = x_i | Y_i = y_i)\n",
    "      \\end{equation*}\n",
    "     \n",
    "     This probability is independent of the\n",
    "    particular position so, for every $i$ and $t$, we can write:  \n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(X_i = w_j | Y_i = c_k) = P(X_{t} = w_j | Y_{t} = c_k)\n",
    "     \\end{equation*}\n",
    "\n",
    "These conditional independence assumptions are crucial to allow\n",
    "efficient inference, as it will be described.\n",
    "\n",
    "\n",
    "### Table summary\n",
    "\n",
    " The distributions that define the HMM model are summarized in the following table\n",
    "\n",
    "<img src=\"./images_for_notebooks/day_2/Hmm_table.png\" style=\"max-width:100%; width: 75%\">\n",
    "\n",
    "### Joint distribution $P(X,Y)$\n",
    "\n",
    "The joint probability of a first order HMM can be written as follows:\n",
    "$$\n",
    "P(X_1=x_1,\\ldots,X_N=x_N,Y_1=y_1,\\ldots,Y_N=y_N)= \n",
    "P_{\\mathrm{init}}(y_1|\\text{ start}) \n",
    "\\cdot\n",
    "\\left(\n",
    "\\prod_{i=1}^{N-1} P_{\\mathrm{trans}}(y_{i+1}|y_i)\n",
    "\\right)\n",
    "\\times\n",
    "P_{\\mathrm{final}}(\\text{ stop}|y_N)\n",
    "\\cdot \n",
    "\\prod_{i=1}^{N} P_{\\mathrm{emiss}}(x_i|y_i)\n",
    "$$\n",
    "\n",
    "#### Example: computing the probability of a pair $(x,y)$\n",
    "the probability of an HMM for the first training instance of Example 2.1, which is \n",
    "\n",
    "$$\n",
    "(x,y) = ([\\text{walk}, \\text{walk}, \\text{shop}, \\text{clean}],  [\\text{rainy}, \\text{sunny},\\text{ sunny}, \\text{sunny}])\n",
    "$$\n",
    "can be computed as\n",
    "\n",
    "$$\n",
    "P(X_1=x_1,\\ldots,X_4=x_4,Y_1=y_1,\\ldots,Y_4=y_4)= \n",
    "P_{\\text{init}}(\\text{rainy}|\\text{ start}) \n",
    "\\cdot\n",
    "P_{\\mathrm{trans}}(\\text{ sunny}|\\text{ rainy}) \n",
    "\\cdot\n",
    "P_{\\mathrm{trans}}(\\text{ sunny}|\\text{ sunny}) \n",
    "\\cdot\n",
    "P_{\\mathrm{trans}}(\\text{ sunny}|\\text{ sunny}) \n",
    "\\cdot\n",
    "P_{\\mathrm{final}}(\\text{ stop}|\\text{ sunny}) \n",
    "\\cdot\n",
    "P_{\\mathrm{emiss}}(\\text{ walk}|\\text{ rainy}) \n",
    "\\cdot\n",
    "P_{\\mathrm{emiss}}(\\text{ walk}|\\text{ sunny}) \n",
    "\\cdot\n",
    "P_{\\mathrm{emiss}}(\\text{ shop}|\\text{ sunny})\n",
    "\\cdot\n",
    "P_{\\mathrm{emiss}}(\\text{ clean}|\\text{ sunny}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Maximum Likelihood Training\n",
    "\n",
    "We have seen how to compute the probability of a pair $(x,y)$ given the probabilities $P_{\\text{init}}, P_{\\text{trans}},P_{\\text{final}},P_{\\text{emiss}}$.\n",
    "\n",
    "Now we will study how to find the parameters that define $P_{\\text{init}}, P_{\\text{trans}},P_{\\text{final}},P_{\\text{emiss}}$. We will refer to the set of parameters as $\\theta$.\n",
    "\n",
    "Given a dataset $\\mathcal{D}_L$, we will try to find the parameters $\\theta$ that maximize the log likelihood function:\n",
    "\n",
    "$$\n",
    "\\log \\prod_{m=1}^M P_{\\theta} (X=x^m,Y=y^m) =  \\sum_{m=1}^M  \\log P_{\\theta} (X=x^m,Y=y^m)\n",
    "$$\n",
    "\n",
    "where the joint distribution $P_{\\theta} (X=x^m,Y=y^m)$ is given by the formula \n",
    "\n",
    "$$\n",
    "P(X_1=x_1,\\ldots,X_N=x_N,Y_1=y_1,\\ldots,Y_N=y_N)= \n",
    "P_{\\mathrm{init}}(y_1|\\text{ start}) \n",
    "\\cdot\n",
    "\\left(\n",
    "\\prod_{i=1}^{N-1} P_{\\mathrm{trans}}(y_{i+1}|y_i)\n",
    "\\right)\n",
    "\\times\n",
    "P_{\\mathrm{final}}(\\text{ stop}|y_N)\n",
    "\\cdot \n",
    "\\prod_{i=1}^{N} P_{\\mathrm{emiss}}(x_i|y_i)\n",
    "$$\n",
    "\n",
    "In some applications  (such as speech recognition) \n",
    "the observation variables are continuous, hence the emission distributions are real-valued ( e.g. mixtures of Gaussians). In our case, both the state set and the observation set are discrete (and finite), therefore we use\n",
    "multinomial distributions for the emission and \n",
    "transition probabilities. \n",
    "\n",
    "Multinomial distributions are attractive for several reasons: first of\n",
    "all, they are easy to implement; secondly, the maximum likelihood estimation of the parameters has a simple closed form. The parameters are just normalized counts of events that occur in the corpus.\n",
    "\n",
    " Let us define the following\n",
    "quantities, called sufficient statistics, that represent the counts of\n",
    "each event in the corpus:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initial counts:\n",
    "$$C_{\\text{init}}(c_k) = \\sum_{m=1}^M\n",
    "\\mathbb{1} (y^m_1 = c_k)\n",
    "$$\n",
    "\n",
    "- Transition counts: $$\n",
    "C_{\\text{trans}}(c_k,c_l) =\n",
    "\\sum_{m=1}^M  \\sum_{i = 2}^{N}\n",
    "\\mathbb{1} (y^m_i = c_k \\wedge y^m_{i-1} = c_l)\n",
    "$$\n",
    "\n",
    "- Final counts:\n",
    "$$\n",
    "C_{\\text{final}}(c_k) = \\sum_{m=1}^M\n",
    "\\mathbb{1} (y^m_N = c_k)\n",
    "$$\n",
    "\n",
    "- Emission counts:\n",
    "$$\n",
    "C_{\\text{emiss}}(w_j,c_k) = \\sum_{m=1}^M\n",
    "\\sum_{i = 1}^{N}\n",
    "\\mathbb{1} (x^m_i = w_j \\wedge y^m_i = c_k)\n",
    "$$\n",
    "\n",
    "Here $y^m_i$,  the underscript denotes the state index position for a given sequence, and the superscript denotes the sequence index in the dataset, and the same applies for the observations.\n",
    "Note that $\\mathbb{1}$ is an indicator function that has the value 1 when the\n",
    "particular event happens, and zero otherwise. In other words, the previous\n",
    "equations go through the training corpus and count how\n",
    "often each event occurs. For example trainsition counts, counts how many times $c_k$ follows state $c_l$. Therefore, $C_{\\text{trans}}(\\text{ sunny},\\text{ rainy})$ contains the number of times that a sunny day followed a rainy day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for the HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initial counts must sum to the number of sentences  $$ \\sum_{k=1}^K C_{\\text{init}}(c_k) = M$$\n",
    "\n",
    "- Transition counts and Final Counts should sum to the number of tokens: $$\\sum_{k,l=1}^K C_{\\text{trans}}(c_k,c_l)  + \\sum_{k=1}^K C_{\\text{final}}(c_k) = M \\cdot N$$\n",
    "\n",
    "- Emission counts must sum to the number of tokens\n",
    "$$\n",
    "\\sum_{j=1}^J \\sum_{k=1}^K C_{\\text{emiss}}(w_j,c_k) = M \\cdot N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an HMM: Finding the parameters of the distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following formulas specify how to find the parameters of the HMM:\n",
    "\n",
    "$$\n",
    "P_{\\text{init}}(c_k \\,\\vert\\, \\text{start}) = \\frac{C_{\\text{init}}(c_k)}{ \\sum_{k=1}^K\n",
    "C_{\\text{init}} (c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{final}}(\\text{stop} \\,\\vert\\, c_l) = \\frac{C_{\\text{final}}(c_l) }\n",
    "{\\sum_{k=1}^K C_{\\text{trans}}(c_k,c_l) + C_{\\text{final}}(c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{trans}}( c_k \\,\\vert\\, c_l) = \\frac{C_{\\text{trans}}(c_k, c_l) }\n",
    "{\\sum_{p=1}^K C_{\\text{trans}}(c_p,c_l) + C_{\\text{final}}(c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{emiss}} (w_j \\,\\vert\\, c_k) = \\frac{C_{\\text{emiss}} (w_j, c_k) }{\\sum_{q=1}^J C_{\\text{emiss}}(w_q,c_k)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The provided function train supervised from the hmm.py file implements the above parameter estimates.  Run this function given the simple dataset above and look at the estimated probabilities. Are they correct? \n",
    "\n",
    "You can also check the variables ending in  counts instead of  probs to see the raw counts (for example, typing ``hmm.initial_counts`` will show you the raw counts of initial states). How are the counts related to the probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "\n",
    "# Load data\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "# instanciate HMM model using the loaded data\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "\n",
    "# Train the HMM\n",
    "hmm.train_supervised(simple.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Counts:\n",
      "[ 2.  1.] \n",
      "\n",
      "Transition Counts:\n",
      "[[ 2.  0.]\n",
      " [ 2.  5.]] \n",
      "\n",
      "Final Counts:\n",
      "[ 0.  3.] \n",
      "\n",
      "Emission Counts\n",
      "[[ 3.  2.]\n",
      " [ 1.  3.]\n",
      " [ 0.  3.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print \"Initial Counts:\\n\", hmm.initial_counts ,\"\\n\"\n",
    "print \"Transition Counts:\\n\", hmm.transition_counts ,\"\\n\"\n",
    "print \"Final Counts:\\n\", hmm.final_counts ,\"\\n\"\n",
    "print \"Emission Counts\\n\", hmm.emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_probs \n",
      "[ 0.66666667  0.33333333]\n",
      "\n",
      "transition_probs\n",
      "[[ 0.5    0.   ]\n",
      " [ 0.5    0.625]]\n",
      "\n",
      "final_probs\n",
      "[ 0.     0.375]\n",
      "\n",
      "emission_probs\n",
      "[[ 0.75   0.25 ]\n",
      " [ 0.25   0.375]\n",
      " [ 0.     0.375]\n",
      " [ 0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print \"initial_probs \"\n",
    "print hmm.initial_counts / np.sum(hmm.initial_counts)\n",
    "\n",
    "print \"\\ntransition_probs\"\n",
    "print hmm.transition_counts / (np.sum(hmm.transition_counts, 0) + hmm.final_counts)\n",
    "\n",
    "print \"\\nfinal_probs\"\n",
    "print hmm.final_counts / (np.sum(hmm.transition_counts, 0) + hmm.final_counts)\n",
    "\n",
    "print \"\\nemission_probs\"\n",
    "print hmm.emission_counts / np.sum(hmm.emission_counts, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION:\n",
    "\n",
    "**If we stack trainsition and final counts and normalize them we get\n",
    "a proper conditional probability distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transitions_with_final_counts = np.vstack((hmm.transition_counts,\n",
    "                                           hmm.final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.],\n",
       "       [ 2.,  5.],\n",
       "       [ 0.,  3.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions_with_final_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5  ,  0.   ],\n",
       "       [ 0.5  ,  0.625],\n",
       "       [ 0.   ,  0.375]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions_with_final_counts/ np.sum(transitions_with_final_counts,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilities:\n",
      "[ 0.66666667  0.33333333] \n",
      "\n",
      "Transition 'Probabilities':\n",
      "[[ 0.5    0.   ]\n",
      " [ 0.5    0.625]] \n",
      "\n",
      "Final 'Probabilities':\n",
      "[ 0.     0.375] \n",
      "\n",
      "Emission Probabilities\n",
      "[[ 0.75   0.25 ]\n",
      " [ 0.25   0.375]\n",
      " [ 0.     0.375]\n",
      " [ 0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print \"Initial Probabilities:\\n\", hmm.initial_probs ,\"\\n\"\n",
    "print \"Transition 'Probabilities':\\n\", hmm.transition_probs ,\"\\n\"\n",
    "print \"Final 'Probabilities':\\n\", hmm.final_probs ,\"\\n\"\n",
    "print \"Emission Probabilities\\n\", hmm.emission_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding a Sequence\n",
    "\n",
    "**So far we have seen how to train a HMM.**\n",
    "\n",
    "**Now we will focus on, once trained, how to make predictions efficiently with a HMM**\n",
    "\n",
    "\n",
    "Given the learned parameters and a new\n",
    "observation sequence $x = x_1\\ldots x_N$, we want to find the sequence of hidden states $y^* = y_1^* \\ldots y_N^*$ that \"best\" explains it.\n",
    " This is called the **decoding problem**. \n",
    " \n",
    " There are several ways to define what we mean by the \"best\" $y^*$, depending on our goal: for instance, we may want to minimize the probability of error on each hidden\n",
    "variable $Y_i$ (posterior decoding), or we may want to find the best assignment to the sequence $Y_1\\ldots Y_N$ as a whole (viterbi decoding). \n",
    "Therefore, finding the best sequence\n",
    "can be accomplished through different approaches:\n",
    "\n",
    "- ** posterior decoding** or **minimum risk decoding**\n",
    "\n",
    "    This approach selects, at each step $i$, the state that maximizes the conditional probability of $Y_i$ given all the visible sequence. Notice that the state sequence that this approach reaches is not necesary the one that maximizes the probability of the whole sequence and state sequence.\n",
    "    \n",
    "\\begin{equation}\n",
    "y_i^* = \\arg \\max_{y_i \\in \\Lambda} P(Y_i=y_i | X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{equation}\n",
    "\n",
    "- ** Viterbi decoding**\n",
    "\n",
    "    This approach choose the sequence of states that, overall, has the highest probability.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y^* &=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N | X_1=x_1,\\ldots,X_N =x_N)\\nonumber\\\\\n",
    "&=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N, X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Unfolding all state sequences: trellis representation\n",
    "Both previous approaches, viterbi decoding and posterior decoding, rely on dynamic programming and make use of the\n",
    "independence assumptions of the HMM model. Moreover, they use an alternative representation of the HMM called a trellis. \n",
    "\n",
    "A trellis unfolds all possible states for each position and it makes explicit the independence assumption: each position only\n",
    "depends on the previous position. Here, each column represents a position in the sequence and each row represents a possible state. The following figure shows the trellis for $x = \\text{walk walk shop clean}$\n",
    "\n",
    "\n",
    "<img src=\"./images_for_notebooks/day_2/hmm_trellis.png\" style=\"max-width:100%; width: 60%\">\n",
    "\n",
    "\n",
    "\n",
    "Considering the trellis representation, note that we can include the following information:\n",
    "- an initial probability to the arrows that depart from the start symbol;\n",
    "- a final probability} to the arrows that reach the stop symbol\n",
    "- a transition probability to the remaining arrows\n",
    "-  an emission probability to each circle, which is the probability that the observed symbol is emitted by that particular state.\n",
    "\n",
    "\n",
    "###  Posterior decoding\n",
    "\n",
    "picking the highest state posterior for each position $i$ in the sequence:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i^* = \\arg \\max_{y_i \\in \\Lambda} P(Y_i=y_i | X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{equation}\n",
    " \n",
    "Note, however, that this approach does not guarantee that the sequence $y^*=y_1^* \\ldots y_N^*$ will be a\n",
    "valid sequence of the model. For instance, there might be a transition\n",
    "between two of the best state posteriors with probability zero. \n",
    "\n",
    "### Viterbi decoding\n",
    "\n",
    "consists in\n",
    "picking the best global hidden state sequence: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "y^* &=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N | X_1=x_1,\\ldots,X_N =x_N)\\nonumber\\\\\n",
    "&=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N, X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding\n",
    "\n",
    "### Working with scores not probabilities\n",
    "\n",
    "For convenience, we will be working with \n",
    "log-probabilities, rather than probabilities. Therefore, if we associate to each circle and arrow in the trellis a score that corresponds\n",
    "to the log-probabilities above, and if we define the score of a path\n",
    "connecting the ${\\tt start}$ and  ${\\tt stop}$ symbols as\n",
    "the sum of the scores of the circles and arrows it traverses, \n",
    "then the goal of **finding the most likely sequence of states (Viterbi decoding) corresponds to finding the path with the highest score**.\n",
    "\n",
    "\n",
    "\n",
    "The trellis scores are given by the following expressions:\n",
    "\n",
    "- For each state $c_k$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{init}}(c_k) &=&\n",
    "\\log P_{\\mathrm{init}}(Y_{1} = c_k | \\text{start}).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "- For each position $i \\in {1,\\ldots,N-1}$ and each pair of states $c_k$ and $c_l$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{trans}}(i, c_k, c_l) &=&\n",
    "\\log P_{\\mathrm{trans}}(Y_{i+1} = c_k | Y_i = c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "- For each state $c_l$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{final}}(c_l) &=&\n",
    "\\log P_{\\mathrm{final}}(\\text{stop} | Y_N = c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "- For each position $i \\in {1,\\ldots,N}$ and state $c_k$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{emiss}}(i, c_k) &=&\n",
    "\\log P_{\\mathrm{emiss}}(X_i = x_i | Y_i = c_k).\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The score of a path in the trellis is equivalent to the log-probability log P(x, y)\n",
    "\n",
    "Since the joint distribution $P_{\\theta} (X=x^m,Y=y^m)$ is given by the formula \n",
    "\n",
    "$$\n",
    "P(x,y)= \n",
    "P_{\\mathrm{init}}(y_1|\\text{start}) \n",
    "\\left(\n",
    "\\prod_{i=1}^{N-1} P_{\\mathrm{trans}}(y_{i+1}|y_i)\n",
    "\\right)\n",
    "P_{\\mathrm{final}}(\\text{stop}|y_N)\n",
    "\\prod_{i=1}^{N} P_{\\mathrm{emiss}}(x_i|y_i)\n",
    "$$\n",
    "\n",
    "when we apply the logarithm we get a sum of logarithms of 4 terms. Using the score notation defined above we get\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P(x,y)= \\mathrm{score}_{\\mathrm{init}}(y_1) + \\sum_{i=1}^{N-1}\\mathrm{score}_{\\mathrm{trans}}(i, y_i, y_{i-1}) +\n",
    "\\mathrm{score}_{\\mathrm{final}}(c_l) +\n",
    " \\sum_{i=1}^{N} \\mathrm{score}_{\\mathrm{emiss}}(i, y_k) \n",
    "$$\n",
    "\n",
    "Since a path in the trellis is just an assignment of states $y=y_1,\\dots,y_N$ given words $x=x_1,\\dots,x_N$, computing the score of a path is just the sum of scores above. Moreover we have seen this is equivalent to computing the log probability of $(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convince yourself that the score of a path in the trellis (summing over the scores above) is equivalent to the log-probability \n",
    "log P(X = x, Y = y), as defined in Eq. 2.2. Use the given function compute scores on the first training sequence and confirm\n",
    "that the values are correct.\n",
    "You should get the same values as presented below.\n",
    "\n",
    "** Suggestion: use an example of length 5 instead of 4, emission_scores is a matrix of n_rows=len(sequence)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_scores;\n",
      "[-0.40546511 -1.09861229] \n",
      "\n",
      "transition_scores: \n",
      "[[[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]] \n",
      "\n",
      "final_scores:\n",
      "[       -inf -0.98082925] \n",
      "\n",
      "emission_scores:\n",
      "[[-0.28768207 -1.38629436]\n",
      " [-0.28768207 -1.38629436]\n",
      " [-1.38629436 -0.98082925]\n",
      " [       -inf -0.98082925]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lxmls-toolkit/lxmls/sequences/hmm.py:173: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1,:,:] = np.log(self.transition_probs)\n",
      "../lxmls-toolkit/lxmls/sequences/hmm.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos,:] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "../lxmls-toolkit/lxmls/sequences/hmm.py:176: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[1])\n",
    "\n",
    "print \"initial_scores;\\n\", initial_scores, \"\\n\"\n",
    "print \"transition_scores: \\n\",transition_scores, \"\\n\"\n",
    "print \"final_scores:\\n\", final_scores, \"\\n\"\n",
    "print \"emission_scores:\\n\", emission_scores, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice a couple of things:\n",
    "\n",
    "- transition_scores is a matrix of shape (3,2,2), the first dimension corresponds to the len(x)-1\n",
    "    - The same matrix at each position is copied since the HMM is homogeneous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk/rainy walk/rainy shop/rainy clean/sunny "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.train.seq_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Probabilities\n",
      "[[ 0.75   0.25 ]\n",
      " [ 0.25   0.375]\n",
      " [ 0.     0.375]\n",
      " [ 0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print \"Emission Probabilities\\n\", hmm.emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition Probabilities\n",
      "[[ 0.5    0.   ]\n",
      " [ 0.5    0.625]]\n"
     ]
    }
   ],
   "source": [
    "print \"transition Probabilities\\n\", hmm.transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **if emission_scores = log (emission_probabilities) why are there not 3 -inf????**\n",
    "- **Why we save length(x)-1 times the transition_scores??**\n",
    "\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        length = len(sequence.x) # Length of the sequence.\n",
    "        num_states = self.get_num_states() # Number of states of the HMM.\n",
    "\n",
    "        # Initial position.\n",
    "        initial_scores = np.log(self.initial_probs)\n",
    "\n",
    "        # Intermediate position.\n",
    "        # logzero is just -np.inf\n",
    "        emission_scores = np.zeros([length, num_states]) + logzero()\n",
    "        transition_scores = np.zeros([length-1, num_states, num_states]) + logzero()\n",
    "        for pos in xrange(length):\n",
    "            import pdb;pdb.set_trace()\n",
    "            emission_scores[pos,:] = np.log(self.emission_probs[sequence.x[pos], :])\n",
    "            if pos > 0:\n",
    "                transition_scores[pos-1,:,:] = np.log(self.transition_probs)\n",
    "\n",
    "        # Final position.\n",
    "        final_scores = np.log(self.final_probs)\n",
    "\n",
    "        return initial_scores, transition_scores, final_scores, emission_scores\n",
    "        \n",
    "        \n",
    "Could be changed to\n",
    "\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        length = len(sequence.x) # Length of the sequence.\n",
    "        num_states = self.get_num_states() # Number of states of the HMM.\n",
    "\n",
    "        # Initial position.\n",
    "        initial_scores = np.log(self.initial_probs)\n",
    "\n",
    "        # Intermediate positions\n",
    "        transition_scores = np.log(self.transition_probs) ## now we don't copy the matrix per position\n",
    "        emission_scores = np.log(self.emission_probs[sequence.x,:])\n",
    "        \n",
    "        # Final position.\n",
    "        final_scores = np.log(self.final_probs)\n",
    "\n",
    "        return initial_scores, transition_scores, final_scores, emission_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Computations in log-domain\n",
    "\n",
    "We will see that the decoding algorithms \n",
    "will need to multiply twice as many probability terms as \n",
    "the length $N$ of the sequence. \n",
    "This may cause underflowing problems \n",
    "when $N$ is large, since the nested multiplication of numbers smaller than 1 may easily become smaller than the machine precision. To avoid that\n",
    "problem,  presents a scaled version of the decoding algorithms that avoids this problem. An alternative, which is widely used, is computing\n",
    "in the log-domain. That is, instead of \n",
    "manipulating probabilities, manipulate log-probabilities (the scores presented above). \n",
    "\n",
    "Every time we need to multiply probabilities, \n",
    "we can sum their log-representations, since:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log(\\exp(a) \\times \\exp(b)) = a+b.\n",
    "\\end{equation}\n",
    "\n",
    "Sometimes, we need to add probabilities. \n",
    "In the log domain, this requires us to compute \n",
    "\n",
    "\\begin{equation}\n",
    "\\log(\\exp(a) + \\exp(b)) = a + \\log(1 + \\exp(b-a)),\n",
    "\\end{equation}\n",
    "\n",
    "where we assume that $a$ is smaller than $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the module ``sequences/log_domain.py.`` This module implements a function ```logsum_pair(logx, logy)``` to add two numbers\n",
    "represented in the log-domain; it returns their sum also represented in the log-domain.\n",
    "\n",
    "The function ```logsum(logv)``` sums all components of an array represented in the log-domain.\n",
    "This will be used later in our decoding algorithms. To observe why this is important, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67569760148\n",
      "7.27073124628\n",
      "67.7034550383\n",
      "677.030677814\n",
      "\n",
      "\n",
      "2.67569760148\n",
      "7.27073124628\n",
      "67.7034550383\n",
      "677.030677814\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10)\n",
    "print np.log(sum(np.exp(a)))\n",
    "print np.log(sum(np.exp(10*a)))\n",
    "print np.log(sum(np.exp(100*a)))\n",
    "print np.log(sum(np.exp(1000*a)))\n",
    "\n",
    "print \"\\n\"\n",
    "from lxmls.sequences.log_domain import logsum\n",
    "print logsum(a)\n",
    "print logsum(10*a)\n",
    "print logsum(100*a)\n",
    "print logsum(1000*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior decoding\n",
    "\n",
    "Posterior decoding consists\n",
    "in picking state with the highest posterior for each position in the sequence independently; for \n",
    "each $i = 1,\\ldots,N$:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i^* = \\text{argmax}_{y_i \\in \\Lambda} P(Y_i=y_i | X = x).\n",
    "\\end{equation}\n",
    "\n",
    "The **sequence posterior distribution** is the probability of a particular\n",
    "hidden state sequence given that we have observed a particular\n",
    "sequence. Moreover, we will be interested in two other posteriors distributions:\n",
    "the **state posterior distribution**, corresponding to the\n",
    "probability of being in a given state in a certain position given the\n",
    "observed sequence; and the \\textbf{transition posterior distribution},\n",
    "which is the probability of making a particular transition, from position $i$ to\n",
    "$i+1$, given the observed sequence. \n",
    "\n",
    "They are formally defined as follows:\n",
    "\n",
    "- Sequence  Posterior\n",
    "$$P(Y=y|X=x) = \\frac{P(X=x,Y=y)}{P(X=x)}\n",
    "$$\n",
    "\n",
    "- State Posterior\n",
    "$$\n",
    "P(Y_i=y_i | X=x)\n",
    "$$\n",
    "\n",
    "- Transition Posterior\n",
    "$$\n",
    "P(Y_{i+1}=y_{i+1},Y_i=y_i| X=x)\n",
    "$$\n",
    "\n",
    "\n",
    "### Computing posteriors involves beeing able to compute $P(X=x)$\n",
    "To compute the posteriors, a first step is to be able to compute the \n",
    "likelihood of\n",
    "the sequence $P(X=x)$, which corresponds to summing the probability of all\n",
    "possible hidden state sequences.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Likelihood\\!:}\\;\\;\\;\\; P(X=x) = \\displaystyle \\sum_{y \\in \\Lambda^N} P(X=x,Y=y).\n",
    "\\end{equation}\n",
    "\n",
    "The number of possible hidden state sequences is exponential in the\n",
    "length of the sequence ($|\\Lambda|^N$),\n",
    " which makes the sum over all of them hard. \n",
    " In our simple\n",
    " example, there are $2^4 = 16$ paths, which we can actually explicitly enumerate\n",
    " and calculate their probability using Equation of the joint probability $P(x,y)$. But this is as far as it goes: for example, for Part-of-Speech\n",
    " tagging with a small tagset of 12 tags and a medium size\n",
    " sentence of length 10, there are $12^{10} = 61 917 364 224$ such\n",
    " paths. \n",
    " \n",
    " Yet, we must be able to compute this sum (sum over $y \\in \\Lambda^N$) to compute the above likelihood\n",
    "formula; this is called the inference problem. For sequence models, there is a well known dynamic programming algorithm,\n",
    "the **Forward-Backward** (FB) algorithm, which allows the computation\n",
    "to be performed in linear time, The runtime is linear with respect\n",
    "to the sequence length. More precisely, \n",
    "the runtime is $O(N|\\Lambda|^2)$. \n",
    "A naive enumeration would cost $O(|\\Lambda|^N)$.\n",
    "\n",
    "The FB algorithm relies on the independence of previous states\n",
    "assumption, which  \n",
    "is illustrated in the trellis view by having arrows only between consecutive states. \n",
    "The FB algorithm defines two auxiliary probabilities, the forward probability and the backward probability. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient forward probability computation\n",
    "\n",
    "The forward probability represents the probability that in position\n",
    "$i$ we are in state $Y_i = c_k$ and that we have observed $x_1,\\ldots,x_i$\n",
    "up to that position. Therefore, its mathematical expression is:\n",
    "\\begin{equation}\n",
    "\\mathbf{Forward \\ Probability\\!:}\\;\\;\\;\\;  \\mathrm{forward}(i, c_k) = P(Y_i = c_k, X_1=x_1,\\ldots, X_i = x_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\mathrm{forward}(i, c_k)$ using all the forward computations \\{$\\mathrm{forward}(i -1, c)$ for $c \\in \\Lambda$\\}. In order to facilitate the notation of the following argument we will denote by $x_{i:j}$  the assignemnt $X_i = x_i, \\dots, X_j = x_j$. Therefore we can write   $\\mathrm{forward}(i, y_i) $ as $P( y_i, x_{1:i } ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( y_i, x_{1:i } ) =  \\sum_{y_{i-1} \\in \\Lambda} P( y_i ,y_{i-1}, x_{1:i } )  =  \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) \\cdot P(y_i  | y_{i-1},  x_{1:i-1 }) \\cdot P(y_{i-1},  x_{1:i-1 })  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the **Observation independence** and the **Independence of previous states** properties of the first order HMM we have $P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) = P( x_i  | y_i) $ and $P(y_i  | y_{i-1},  x_{1:i-1 })  = P(y_i  | y_{i-1})  $. Therefore the previous equation can be written, \n",
    "for $i \\in \\{2,\\dots,N\\}$ (where $N$ is the length of the sequence), as \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathrm{forward}(i, y_i)  = \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i, ) \\cdot P(y_i  | y_{i-1}) \\cdot \\mathrm{forward}(i-1, y_{i-1})   \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The previous equation proves that  the forward probability can be defined by the\n",
    "following recurrence rule: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{forward}(1, c_k)&=& P_{\\text{init}}(c_k|\\text{start}) \\times P_{\\mathrm{emiss}}(x_1 | c_k)\n",
    " \\\\\n",
    " \\mathrm{forward}(i, c_k) &=& \\left(  \\sum_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{forward}(i-1, c_l) \\right) \\times P_{\\mathrm{emiss}}(x_i | c_k) \n",
    " \\\\\n",
    "  \\mathrm{forward}(N+1, \\text{stop}) &=& \\sum_{c_l \\in \\Lambda} P_{\\text{final}}(\\text{ stop} | c_l) \\times \\mathrm{forward}(N, c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Using the forward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{forward}(N+1, \\text{ stop}).\n",
    "\\end{equation}\n",
    "\n",
    "Although the forward probability is enough to calculate the likelihood of a given sequence, we will also need the backward probability to calculate the state posteriors. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient backward probability computation\n",
    "\n",
    "\n",
    "\n",
    "The backward probability is similar to the forward probability, but operates in the inverse direction.\n",
    "It represents the probability of observing $x_{i+1},\\ldots,x_N$ from position $i+1$ up to $N$, given that at position $i$ we are at state $Y_i = c_l$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Backward \\ Probability\\!:}\\;\\;\\;\\;  \\text{backward}(i, c_l) = P(X_{i+1}=x_{i+1},\\ldots, X_N=x_N | Y_i = c_l).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\text{backward}(i, c_k)$ using all the backward computations $\\text{backward}(i +1, c)$ for $c \\in \\Lambda$.\n",
    "\n",
    "Therefore we can write   $\\text{backward}(i, y_i) $ as $P( x_{i+1:N} | y_i ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( x_{i+1:N} | y_i ) =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+1:N}, y_{i+1} | y_i)  =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+2:N} | y_i, y_{i+1}, x_{i+1}) \n",
    "   P( x_{i+1}, |  y_{i+1},  y_{i}) P( y_{i+1} | y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Using the previous equation we have proved that the backward probability can be defined by the following recurrence rule:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{backward}(N, c_l) &=& P_{\\text{final}}(\\text{stop} | c_l)  \\\\\n",
    "\\text{backward}(i, c_l) &=&  \\displaystyle \\sum_{c_k \\in \\Lambda} P_{\\text{trans}}(c_k | c_l) \\times \n",
    "\\text{backward}(i+1, c_k) \\times P_{\\text{emiss}}(x_{i+1} | c_k) \n",
    " \\\\\n",
    "  \\mathrm{backward}(0, \\text{start}) &=& \\sum_{c_k \\in \\Lambda} P_{\\mathrm{init}}(c_k | \\text{ start}) \\times \\mathrm{backward}(1, c_k) \\times P_{\\mathrm{emiss}}(x_{1} | c_k).\n",
    " \\end{eqnarray}\n",
    "\n",
    "Using the backward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{backward}(0, \\text{start}).\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward backward algorithm\n",
    "\n",
    "We have seen how we can compute the probability of a sequence $x$ using the the forward and backward probabilities by computing  $\\mathrm{forward}(N+1, \\text{ stop})$ and $ \\mathrm{backward}(0, \\text{ start})$ respectively. Moreover,  the probability of a sequence $x$ can be computed with both forward and backward probabilities at a particular position $i$. \n",
    "\n",
    "The probability of a  given sequence $x$ at any position $i$ in the sequence can be computed\n",
    "as follows:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  P(X=x) &=& \n",
    "  \\sum_{c_k \\in \\Lambda} P(X_1=x_1,\\ldots, X_N=x_N,Y_i=c_k)\\nonumber\\\\\n",
    "  & =&\n",
    "  \\sum_{c_k \\in \\Lambda} \n",
    "  \\underbrace{P(X_1=x_1,\\ldots, X_i=x_i, Y_i=c_k)}_{\\mathrm{forward}(i,c_k)} \\times \n",
    "  \\underbrace{P(X_{i+1}=x_{i+1},\\ldots, X_N=x_N| Y_i=c_k)}_{\\mathrm{backward}(i,c_k)}\\nonumber\\\\\n",
    "  &=& \\sum_{c_k \\in \\Lambda} \\mathrm{forward}(i,c_k) \\times \\mathrm{backward}(i,c_k).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "This equation will work for any choice of $i$. Although redundant, this fact is useful when implementing an\n",
    "HMM as a sanity check that the computations are being performed\n",
    "correctly, since one can compute this expression for several $i$; they should all yield the same value. \n",
    "\n",
    "The following pseudocode shows the the forward backward algorithm. \n",
    "\n",
    "<img src=\"./images_for_notebooks/day_2/fb_alg.png\"  style=\"max-width:100%; width: 50%\">\n",
    "\n",
    "The reader can notice that the $forward$ and $backward$ computations in the algorithm make use of $P_{emiss}$ and $P_{trans}$. There are a couple of details that should be taken into account if the reader wants to understand the algorithm using scores instead of probabilities.\n",
    "\n",
    "\n",
    "- $forward(i,\\hat{c})$  is computed using $P_{emiss}(x_i | \\hat{c})$ which does not depend on the sum over all possible states $c_k \\in  \\Lambda $. Therefore when taking the logarithm of the sum over all possible states the recurrence of the forward computations can be split as a sum of two logarithms.\n",
    "\n",
    "\n",
    "- $backward(i,\\hat{c})$  is computed using $ P_{\\text{trans}}(c_k | \\hat{c} )$ and $P_{\\text{emiss}}(x_{i+1} | c_k) $ both of  which  depend on $c_k$. Therefore when taking the logarithm of the sum the expression cannot be split as a sum of logarithms.\n",
    "\n",
    "\n",
    "\n",
    "Given the forward and backward probabilities, one can compute both the state\n",
    "and transition posteriors as follows:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{State \\ Posterior\\!:}\\;\\;\\;\\;  & P(Y_i = y_i| X=x) = \\frac{\\mathrm{forward}(i, y_i) \\times \n",
    " \\mathrm{backward}(i, y_i)}{P(X=x)}\\\\\n",
    " \\mathbf{Transition \\ Posterior\\!:}\\;\\;\\;\\; &\n",
    " P(Y_i = y_i, Y_{i+1} = y_{i+1} | X=x)= \\nonumber\\\\\n",
    " &\n",
    "   \\frac{\\mathrm{forward}(i, y_i) \\times \n",
    "   P_{\\mathrm{trans}}(y_{i+1}|y_i) \\times\n",
    "   P_{\\mathrm{emiss}}(x_{i+1}|y_{i+1}) \\times\n",
    " \\mathrm{backward}(i+1, y_{i+1})}{P(X=x)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical representation of these posteriors is illustrated in the following figure:\n",
    "\n",
    "<img src=\"./images_for_notebooks/day_2/ex_trellis.png\"  style=\"max-width:100%; width: 80%\">\n",
    "\n",
    "On the left it is shown that $\\mathrm{forward}(i, y_i)  \\times \\mathrm{backward}(i, y_i)$ returns the sum of all paths that contain the state $y_i$, weighted by $P(X=x)$; on the right we can see that \n",
    "\n",
    "$$\\mathrm{forward}(i, y_i) \\times P_{\\mathrm{trans}}(y_{i+1}|y_i) \\times P_{\\mathrm{emiss}}(x_{i+1}|y_{i+1}) \\times \\mathrm{backward}(i+1, y_{i+1})$$\n",
    "\n",
    "returns the same for all paths containing the edge from $y_i$ to $y_{i+1}$. Thus, these posteriors can be seen as the ratio of the number of paths that contain the given state or transition (weighted by $P(X=x)$) and the number of possible paths in the graph marginal.\n",
    "\n",
    "As a practical example, given that the person performs the sequence of actions $\\text{ walk} \\text{ walk} \\text{ shop} \\text{ clean}$, we want to know the probability of having been raining in the second day. The state posterior probability for this event can be seen as the probability that the sequence of actions above was generated by a sequence of weathers and where it was raining in the second day. In this case, the possible sequences would be all the sequences which have {\\tt rainy} in the second position.\n",
    "\n",
    "\n",
    "Using the state posteriors, we are ready to perform posterior\n",
    "decoding. \n",
    "The strategy is to compute the state posteriors \n",
    "for each position $i \\in \\{1,\\ldots,N\\}$\n",
    "and each state $c_k \\in \\Lambda$, and \n",
    "then pick the arg-max at each position:\n",
    "\n",
    "$$\n",
    "{\\widehat y_i} := \\text{argmax}_{y_i \\in \\Lambda} P(Y_i=y_i| X=x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the hmm class\n",
    "\n",
    "The HMM class inherits from SequenceClassifier\n",
    "\n",
    "In the following exercise we will use the run_forward function which \n",
    "\n",
    "    def run_forward(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "            length = np.size(emission_scores, 0) # Length of the sequence.\n",
    "            num_states = np.size(initial_scores) # Number of states.\n",
    "\n",
    "            # Forward variables.\n",
    "            forward = np.zeros([length, num_states]) + logzero()\n",
    "\n",
    "            # Initialization.\n",
    "            forward[0,:] = emission_scores[0,:] + initial_scores\n",
    "\n",
    "            # Forward loop.\n",
    "            for pos in xrange(1,length):\n",
    "                for current_state in xrange(num_states):\n",
    "                    # Note the fact that multiplication in log domain turns a sum and sum turns a logsum\n",
    "                    forward[pos, current_state] = \\\n",
    "                            logsum(forward[pos-1, :] + transition_scores[pos-1, current_state, :])\n",
    "                    forward[pos, current_state] += emission_scores[pos, current_state]\n",
    "\n",
    "            # Termination.\n",
    "            log_likelihood = logsum(forward[length-1,:] + final_scores)\n",
    "\n",
    "            return log_likelihood, forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.5 \n",
    "\n",
    "Run the provided forward-backward algorithm on the first train sequence.\n",
    "Observe that both the forward and the backward passes give the same log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[walk/rainy walk/sunny shop/sunny clean/sunny ,\n",
       " walk/rainy walk/rainy shop/rainy clean/sunny ,\n",
       " walk/sunny shop/sunny shop/sunny clean/sunny ]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.train.seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward:\n",
      "[[-0.69314718 -2.48490665]\n",
      " [-1.67397643 -2.58334672]\n",
      " [-3.75341798 -2.94017562]\n",
      " [       -inf -4.08740307]] \n",
      "\n",
      "\n",
      " Log-Likelihood = -5.06823232601\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, forward = hmm.decoder.run_forward(initial_scores, transition_scores,final_scores, emission_scores)\n",
    "print \"forward:\\n\", forward, \"\\n\"\n",
    "print '\\n Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a =hmm.decoder.run_forward(initial_scores, transition_scores,final_scores, emission_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward :\n",
      "[[-4.41863845 -5.73879301]\n",
      " [-3.67819455 -3.88249502]\n",
      " [-2.65480569 -2.43166214]\n",
      " [       -inf -0.98082925]] \n",
      "\n",
      "Log-Likelihood = -5.06823232601\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, backward = hmm.decoder.run_backward(initial_scores, transition_scores, final_scores, emission_scores)\n",
    "print \"backward :\\n\", backward, \"\\n\"\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the node posteriors for the first training sequence (use the provided compute posteriors function), \n",
    "and look at the output. Note that the state posteriors are a proper probability distribution \n",
    "(the lines of the result sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "state_posteriors, transition_posteriors, log_likelihood = hmm.compute_posteriors(initial_scores, transition_scores, final_scores, emission_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95738152  0.04261848]\n",
      " [ 0.75281282  0.24718718]\n",
      " [ 0.26184794  0.73815206]\n",
      " [ 0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print state_posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 2.7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the posterior decode on the first test sequence, and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny  \n",
      "\n",
      "Truth test 0:\n",
      "\twalk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0  ])\n",
    "print \"Prediction test 0:\\n\\t\", y_pred, \"\\n\"\n",
    "print \"Truth test 0:\\n\\t\", simple.test.seq_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the second test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 1:\n",
      "clean/rainy walk/rainy tennis/rainy walk/rainy \n",
      "Truth test 1:\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lxmls-toolkit/lxmls/sequences/sequence_classifier.py:78: RuntimeWarning: invalid value encountered in subtract\n",
      "  state_posteriors[pos,:] -= log_likelihood\n",
      "../lxmls-toolkit/lxmls/sequences/sequence_classifier.py:91: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  transition_posteriors[pos, state, prev_state] -= log_likelihood\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "# There are nan values in the backward and forward probabilites caused by\n",
    "# not having observed tennis\n",
    "\n",
    "print \"Prediction test 1:\"\n",
    "print y_pred\n",
    "print \"Truth test 1:\"\n",
    "print simple.test.seq_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong?\n",
    "\n",
    "**Note the observations for the second test sequence: the observation tennis was never seen at training time**, so the probability for it will be zero (no matter what state). This will make all possible state sequences have zero probability. As seen in the previous lecture, this is a problem with generative models, which can be corrected using smoothing (among other options).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the train supervised method to add smoothing:\n",
    "```\n",
    "   def train_supervised(self,sequence_list, smoothing):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0 with smoothing:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Truth test 0:\n",
      "\twalk/rainy walk/sunny shop/sunny clean/sunny \n",
      "\n",
      "\n",
      "Prediction test 1 with smoothing:\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "Truth test 1:\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print \"Prediction test 0 with smoothing:\"\n",
    "print \"\\t\",y_pred \n",
    "print \"Truth test 0:\"\n",
    "print \"\\t\",simple.test.seq_list[0]\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print \"\\n\"\n",
    "print \"Prediction test 1 with smoothing:\"\n",
    "print \"\\t\",y_pred\n",
    "print \"Truth test 1:\"\n",
    "print \"\\t\",simple.test.seq_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi decoding\n",
    "\n",
    "**Viterbi decoding** consists in\n",
    "picking the best global hidden state sequence  $\\widehat{y}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{y} = \\text{argmax}_{y \\in \\Lambda^N} P(Y=y|X=x) = \\text{argmax}_{y \\in \\Lambda^N} P(X=x,Y=y).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The Viterbi algorithm  is very similar to the forward procedure of the FB algorithm,\n",
    "making use of the same trellis structure to efficiently represent the exponential number of sequences without prohibitive computation costs. In fact, the only\n",
    "difference from the forward-backward algorithm is in the recursion where **instead of summing over all possible hidden states, we take their maximum**.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Viterbi }\\;\\;\\;\\;  \\mathrm{viterbi}(i, \\pmb{x}, y_i) = \\max_{y_1...y_{i-1}} P(Y_1=y_1,\\ldots Y_i = y_i , X_1=x_1,\\ldots, X_i=x_i)\n",
    "\\end{equation}\n",
    "\n",
    "The Viterbi trellis represents the path with maximum probability in\n",
    "position\n",
    "$i$ when we are in state $Y_i=y_i$ and that we have observed $x_1,\\ldots,x_i$\n",
    "up to that position. The Viterbi algorithm is defined by the\n",
    "following recurrence of the viterbi values: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{viterbi}(1, \\pmb{x}, c_k) &=& P_{\\mathrm{init}}(c_k|\\text{ start}) \\times \n",
    "P_{\\mathrm{emiss}}(x_1 | c_k)\n",
    " \\\\\n",
    " \\mathrm{viterbi}(i, \\pmb{x}, c_k) &=& \\left( \\displaystyle \\max_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{viterbi}(i-1, \\pmb{x}, c_l) \\right) \\times P_{\\mathrm{emiss}}(x_i | c_k)\n",
    "  \\\\\n",
    "  \\mathrm{viterbi}(N+1, \\pmb{x}, \\text{ stop}) &=& \\max_{c_l \\in \\Lambda} P_{\\mathrm{final}}(\\text{ stop} | c_l) \\times \\mathrm{viterbi}(N, \\pmb{x}, c_l)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Once the viterbi value at the last position ``viterbi(N,x,c_l)`` is computed the algorithm can backtrack using the following recurrence\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{backtrack}(N+1, \\pmb{x}, \\text{ stop}) &=& \\text{argmax}_{c_l \\in \\Lambda} P_{\\mathrm{final}}(\\text{ stop} | c_l) \\times \\mathrm{viterbi}(N,\\pmb{x}, c_l).\n",
    " \\\\\n",
    "\\mathrm{backtrack}(i,\\pmb{x}, c_k) &=& \\left( \\displaystyle \\text{argmax}_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{viterbi}(i-1,\\pmb{x}, c_l) \\right) \n",
    " \\end{eqnarray}\n",
    "\n",
    "The following  pseudo code  is the Viterbi algorithm.\n",
    "Note the similarity with the forward algorithm.\n",
    "The only differences are:\n",
    "\n",
    "- Maximizing instead of summing;\n",
    "- Keeping the argmax's to backtrack.\n",
    "\n",
    "<img src=\"./images_for_notebooks/day_2/viterbi.png\"  style=\"max-width:100%; width: 80%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # THIS CANT GO INTO THE STUDENT VERSION\n",
    "    def run_viterbi(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "\n",
    "        length = np.size(emission_scores, 0) # Length of the sequence.\n",
    "        num_states = np.size(initial_scores) # Number of states.\n",
    "\n",
    "        # Viterbi variables.\n",
    "        viterbi = np.zeros([length, num_states]) + logzero()\n",
    "        backtrack = np.zeros([length, num_states], dtype=int)\n",
    "\n",
    "        # Initialization.\n",
    "        viterbi[0,:] = emission_scores[0,:] + initial_scores\n",
    "\n",
    "        # viterbi loop.\n",
    "        for pos in xrange(1,length):\n",
    "            for current_state in xrange(num_states):\n",
    "                # Note the fact that multiplication in log domain turns a sum and sum turns a logsum\n",
    "                viterbi_score = viterbi[pos-1, :] + transition_scores[pos-1, current_state, :]\n",
    "                viterbi[pos, current_state] = np.max(viterbi_score)\n",
    "                viterbi[pos, current_state] += emission_scores[pos, current_state]\n",
    "                backtrack[pos, current_state] = np.argmax(viterbi_score)\n",
    "\n",
    "        best_score = np.max(viterbi[-1, :] + final_scores)\n",
    "        \n",
    "        best_path = np.zeros(length, dtype=int)\n",
    "        best_path[-1] = np.argmax(viterbi[-1, :] + final_scores)\n",
    "\n",
    "        for pos in xrange(length-2,-1,-1):\n",
    "            #best_path[pos] = int(np.argmax(backtrack[pos+1]))\n",
    "            best_path[pos] = backtrack[pos+1, best_path[pos+1]]\n",
    "\n",
    "        return best_path , best_score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the method  ``run_viterbi`` for performing Viterbi decoding\n",
    "in file ```lxmls/sequences/sequence_classification_decoder.py.```**\n",
    "\n",
    "This method at the moment raises a NotImplementedError\n",
    "\n",
    "    def run_viterbi(self, initial_scores,transition_scores,final_scores,emission_scores):\n",
    "        # Complete Exercise 2.8 \n",
    "        raise NotImplementedError, \"Complete Exercise 2.8\" \n",
    "        # THIS FUNCTION SHOULD RETURN \n",
    "        # best_states, total_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use ```hmm.viterbi_decode``` to predict the sequence of tags for a given sequence of visible states. \n",
    "\n",
    "Notice that ```hmm.viterbi_decode``` ( which can be located in ```sequences/sequence_classifier```) uses the method ```SequenceClassificationDecoder.run_viterbi``` (from the folder ```sequences/sequence_classification_decoder```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk/rainy walk/rainy shop/sunny clean/sunny "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 0 with smoothing:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny  \n",
      "score:\n",
      "\t-6.02050124698\n",
      "\n",
      "A correct implementation of Viterbi decoding Prediction test 0 with smoothing\n",
      "should return:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny \n",
      "score:\n",
      "\t-6.02050124698\n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])\n",
    "print \"Viterbi decoding Prediction test 0 with smoothing:\\n\\t\", y_pred, \"\\nscore:\\n\\t\",score\n",
    "\n",
    "print \"\\nA correct implementation of Viterbi decoding Prediction test 0 with smoothing\"\n",
    "print \"should return:\"\n",
    "print \"\\t\", \"walk/rainy walk/rainy shop/sunny clean/sunny \"\n",
    "print \"score:\"\n",
    "print \"\\t\",-6.02050124698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth test 0:\n",
      "\twalk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "print \"Truth test 0:\\n\\t\", simple.test.seq_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 1 with smoothing:\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "score:\n",
      "\t-11.713974074\n",
      "\n",
      "A correct implementation of Viterbi decoding Prediction test 0 with smoothing\n",
      "should return\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "score:\n",
      "\t-11.713974074\n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[1])\n",
    "print \"Viterbi decoding Prediction test 1 with smoothing:\\n\\t\", y_pred\n",
    "print \"score:\"\n",
    "print \"\\t\",score\n",
    "\n",
    "print \"\\nA correct implementation of Viterbi decoding Prediction test 0 with smoothing\"\n",
    "print \"should return\"\n",
    "print \"\\t\",simple.test.seq_list[1] \n",
    "print \"score:\"\n",
    "print \"\\t\",-11.713974074"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech (PoS) tagging is one of the most important NLP tasks. The\n",
    "task is to assign each word a grammatical category, or Part-of-Speech, such as noun,\n",
    "verb, adjective,... Recalling the defined notation, $\\Sigma$ is a \n",
    "vocabulary of word types, and \n",
    "$\\Lambda$ is the set of Part-of-Speech tags.\n",
    "\n",
    "In English, using the Penn Treebank (PTB) corpus , the current\n",
    "state of the art for part of speech tagging is around 97\\% for a\n",
    "variety of methods.\n",
    "\n",
    "In the rest of this class we will use a subset of the PTB corpus, but\n",
    "instead of using the original 45 tags we will use a reduced tag set of\n",
    "12 tags, to make the algorithms faster for the\n",
    "class. In this task, $x$ is a sentence (for example, a sequence of word tokens) and $y$\n",
    "is the sequence of possible PoS tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.pos_corpus as pcc\n",
    "corpus = pcc.PostagCorpus()\n",
    "\n",
    "#path_to_data = path_inside_lxmls_toolkit_student + \"/data/\"\n",
    "path_to_data = \"../lxmls-toolkit/data/\"\n",
    "\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(path_to_data + \"train-02-21.conll\",max_sent_len=15,max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(path_to_data + \"test-23.conll\",max_sent_len=15,max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(path_to_data + \"dev-22.conll\",max_sent_len=15,max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The/det luxury/noun auto/noun maker/noun last/adj year/noun sold/verb 1,214/num cars/noun in/adp the/det U.S./noun "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEKCAYAAACxA4b4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c6a1f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hmm = hmmc.HMM(corpus.word_dict, corpus.tag_dict)\n",
    "hmm.train_supervised(train_seq)\n",
    "hmm.print_transition_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the transition probabilities of the trained model,\n",
    "and see if they match your intuition about the English language \n",
    "(e.g. adjectives tend to come before nouns). Each column is the previous state and row is the current state. Note the high probability of having Noun after Determinant or Adjective, or of having Verb after Nouns or Pronouns, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model using both posterior decoding and Viterbi decoding on\n",
    "both the train and test set, using the methods in class HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_train = hmm.viterbi_decode_corpus(train_seq) \n",
    "posterior_pred_train = hmm.posterior_decode_corpus(train_seq)\n",
    "eval_viterbi_train = hmm.evaluate_corpus(train_seq, viterbi_pred_train)\n",
    "eval_posterior_train = hmm.evaluate_corpus(train_seq, posterior_pred_train)\n",
    "print \"Train Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(eval_posterior_train,eval_viterbi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: Posterior Decode 0.350, Viterbi Decode: 0.509\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq) \n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq) \n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq,viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq,posterior_pred_test) \n",
    "print \"Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(\n",
    "    eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What do you observe? \n",
    "\n",
    "Remake the previous exercise but now train the HMM using smoothing.\n",
    "Try different values (0,0.1,0.01,1) and report the results on the train and \n",
    "development set. (Use function pick best smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing 10.000000 --  Train Set Accuracy: Posterior Decode 0.731, Viterbi Decode: 0.691\n",
      "Smoothing 10.000000 -- Test Set Accuracy: Posterior Decode 0.712, Viterbi Decode: 0.675\n",
      "Smoothing 1.000000 --  Train Set Accuracy: Posterior Decode 0.887, Viterbi Decode: 0.865\n",
      "Smoothing 1.000000 -- Test Set Accuracy: Posterior Decode 0.818, Viterbi Decode: 0.792\n",
      "Smoothing 0.100000 --  Train Set Accuracy: Posterior Decode 0.968, Viterbi Decode: 0.965\n",
      "Smoothing 0.100000 -- Test Set Accuracy: Posterior Decode 0.851, Viterbi Decode: 0.842\n",
      "Smoothing 0.000000 --  Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n",
      "Smoothing 0.000000 -- Test Set Accuracy: Posterior Decode 0.370, Viterbi Decode: 0.526\n"
     ]
    }
   ],
   "source": [
    "best_smoothing = hmm.pick_best_smoothing(train_seq, dev_seq, [10,1,0.1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Smoothing 0.100000 -- Test Set Accuracy: Posterior Decode 0.837, Viterbi Decode: 0.827\n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(train_seq, smoothing=best_smoothing)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test) \n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)\n",
    "print \"Best Smoothing %f -- Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(best_smoothing,eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform some error analysis to understand were the errors are coming from. \n",
    "\n",
    " You can start by visualizing the confusion matrix (true tags vs predicted tags). \n",
    "\n",
    "You should get something like what is shown in Figure 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd35410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lxmls.sequences.confusion_matrix as cm\n",
    "import matplotlib.pyplot as plt\n",
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test, len(corpus.tag_dict), hmm.get_num_states()) \n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states()), 'Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (OPTIONAL) Unsupervised Learning of HMMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you have made it so far you are awesome! \n",
    "\n",
    "> Don't worry the next couple of exercices do not require to actually code anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next address the problem of unsupervised learning. In this setting, \n",
    "we are not given any labeled data.\n",
    "\n",
    "All we get to see is a set of natural language sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the method to update the counts given the state and transition posteriors\n",
    "\n",
    "```\n",
    "def update_counts(self, sequence, state_posteriors, transition_posteriors):\n",
    "```\n",
    "\n",
    "Look at the code for EM algorithm in file ```sequences/hmm.py``` and check it for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def train_EM(self, dataset, smoothing=0, num_epochs=10, evaluate=True):\n",
    "        self.initialize_random()\n",
    "\n",
    "        if evaluate:\n",
    "            acc = self.evaluate_EM(dataset)\n",
    "            print \"Initial accuracy: %f\"%(acc)\n",
    "\n",
    "        for t in xrange(1, num_epochs):\n",
    "            #E-Step\n",
    "            total_log_likelihood = 0.0\n",
    "            self.clear_counts(smoothing)\n",
    "            for sequence in dataset.seq_list:\n",
    "                # Compute scores given the observation sequence.\n",
    "                initial_scores, transition_scores, final_scores, emission_scores = \\\n",
    "                    self.compute_scores(sequence)\n",
    "\n",
    "                state_posteriors, transition_posteriors, log_likelihood = \\\n",
    "                    self.compute_posteriors(initial_scores,\n",
    "                                            transition_scores,\n",
    "                                            final_scores,\n",
    "                                            emission_scores)\n",
    "                self.update_counts(sequence, state_posteriors, transition_posteriors)\n",
    "                total_log_likelihood += log_likelihood\n",
    "\n",
    "            print \"Iter: %i Log Likelihood: %f\"%(t, total_log_likelihood)\n",
    "            #M-Step\n",
    "            self.compute_parameters()\n",
    "            if evaluate:\n",
    "                 ### Evaluate accuracy at this iteration\n",
    "                acc = self.evaluate_EM(dataset)\n",
    "                print \"Iter: %i Accuracy: %f\"%(t,acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 20 epochs of the EM algorithm for part of speech induction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm.train_EM(train_seq, 0.1, 20, evaluate=True)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq) \n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test, len(corpus.tag_dict), hmm.get_num_states())\n",
    "\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states()), 'Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
