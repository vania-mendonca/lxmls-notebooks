{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../lxmls-toolkit')\n",
    "import lxmls\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "Go to **```lxmls/deep\\_learning/mlp.py:class NumpyMLP```**  and check the function **```grads()```** and complete the code of the NumpyMLP class with the Backpropagation recursion that we just saw.\n",
    "Once you are done. Try different network geometries by increasing the number of\n",
    "layers and layer sizes e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "scr = srs.SentimentCorpus(\"books\")\n",
    "\n",
    "train_x = scr.train_X.T\n",
    "train_y = scr.train_y[:, 0]\n",
    "test_x = scr.test_X.T\n",
    "test_y = scr.test_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "# Load mlp and sgd from toolit\n",
    "from lxmls import deep_learning \n",
    "from lxmls.deep_learning import mlp\n",
    "import lxmls.deep_learning.sgd as sgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13989L, 1600L), (1600L,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 100,100, 2]\n",
    "actvfunc = ['sigmoid', 'sigmoid','softmax'] \n",
    "# Instantiate model\n",
    "mlp      = mlp.NumpyMLP(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_iter = 5\n",
    "bsize  = 5\n",
    "lrate  = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5L,)\n",
      "(2L, 5L)\n",
      "[[  9.99338655e-01   9.98822251e-01   9.98398580e-01   9.98609034e-01\n",
      "    9.97784239e-01]\n",
      " [  6.61344905e-04   1.17774889e-03   1.60141965e-03   1.39096620e-03\n",
      "    2.21576076e-03]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f5c09b9e6550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0macc_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\sgd.pyc\u001b[0m in \u001b[0;36mSGD_train\u001b[1;34m(model, n_iter, bsize, lrate, train_set, batch_up, n_batch, devel_set, model_dbg)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mbatch_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[1;31m# INFO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\rBatch %d/%d (%d%%) \"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\sgd.pyc\u001b[0m in \u001b[0;36mbatch_up\u001b[1;34m(batch_x, batch_y)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# Get gradients for each layer and this batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Get gradients for each layer and this batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mnabla_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;31m# Update each parameter with SGD rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\mlp.pyc\u001b[0m in \u001b[0;36mgrads\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                     \u001b[0mnon_linear_error\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex2onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mactivations_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[0mprod_nle_activation\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_linear_error\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations_n_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\mlp.pyc\u001b[0m in \u001b[0;36mindex2onehot\u001b[1;34m(index, N)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mOutput\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0monehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test  = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "print \"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\" % (geometry, acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load mlp and sgd from toolit\n",
    "from lxmls import deep_learning \n",
    "from lxmls.deep_learning import mlp\n",
    "import lxmls.deep_learning.sgd as sgd\n",
    "\n",
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 300, 2]\n",
    "actvfunc = ['sigmoid', 'softmax'] \n",
    "# Instantiate model\n",
    "mlp      = mlp.NumpyMLP(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test  = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "print \"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\" % (geometry, acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2 \n",
    "#### Begin Exercise 5.2\n",
    "\n",
    "Get in contact with Theano. Learn the difference between a symbolic\n",
    "representation and a function. Start by implementing the first layer of our\n",
    "previous MLP in Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy code\n",
    "x        = test_x             # Test set \n",
    "W1, b1   = mlp.params[:2]     # Weights and bias of fist layer \n",
    "z1       = np.dot(W1, x) + b1 # Linear transformation\n",
    "tilde_z1 = 1/(1+np.exp(-z1))  # Non-linear transformation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano code. \n",
    "# NOTE: We use undescore to denote symbolic equivalents to Numpy variables. \n",
    "# This is no Python convention!.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "_x = T.matrix('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this variable does not have any particular value, nor a space\n",
    "reserved in memory for it. It contains just a symbolic definition of what the\n",
    "variable can contain. The particular values will be given when we use it to\n",
    "compile a function. \n",
    "\n",
    "We could actually use the same definition format to define the weights and give\n",
    "their particular values as inputs to the compiled function. However, since we\n",
    "will be using a more complicated format in later exercises, we will use it here\n",
    "as well. The **```shared```** class allows to define variables that are shared\n",
    "across functions. They are also given a concrete value so that we do not need\n",
    "to give it for each function call. This format is therefore ideal for the\n",
    "weights of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_W1 = theano.shared(value=W1, name='W1', borrow=True) \n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets describe the operations we want to do with the variables. Again only\n",
    "symbolically. This is done by replacing our usual operations by Theano symbolic\n",
    "ones when necessary e. g. the internal product dot() or the sigmoid. Some\n",
    "operations like e.g. $+$ are automatically recognized by Theano (operator\n",
    "overloading). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_z1            = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1      = T.nnet.sigmoid(_z1)\n",
    "# Keep in mind that naming variables is useful when debugging\n",
    "_z1.name       = 'z1'\n",
    "_tilde_z1.name = 'tilde_z1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging the code it is often useful to print the graph of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perceptron computation graph\n",
    "theano.printing.debugprint(_tilde_z1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep in mind that, until this point, we do not have a\n",
    "function we can use to produce any practical input. In order to obtain this we\n",
    "have to compile this function by calling    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1 = theano.function([_x], _tilde_z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of [ ] for the input variables, even if we just specify one\n",
    "variable. We can now do a test to compare the Numpy and Theano implementations\n",
    "and see that they give the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check Numpy and Theano match\n",
    "if np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX))):\n",
    "    print \"\\nNumpy and Theano Perceptrons are equivalent\"\n",
    "else:\n",
    "    raise ValueError, \"Numpy and Theano Perceptrons are different\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End exercise 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic forward pass\n",
    "In the previous section you have seen how to create symbolic Theano functions\n",
    "with shared parameters. You have thus all you need to implement the whole\n",
    "forward pass of a generic MLP in Theano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.3\n",
    "\n",
    "#### Begin Exercise 5.3\n",
    "\n",
    "Complete the method **```_forward()```** inside of the **```lxmls/deep\\_learning/mlp.py```**, in the class TheanoMLP.\n",
    "\n",
    "Note that this is called only once at the initialization of the\n",
    "class. To debug your implementation put a breakpoint at the \\_\\_init\\_\\_\n",
    "function call. Hint: Note that this is very similar to NumpyMLP.forward().\n",
    "You just need to keep track of the symbolic variable representing the output of\n",
    "the network after each layer is applied and compile the function at the end.\n",
    "After you are finished instantiate a Theano class and check that Numpy and\n",
    "Theano forward pass are the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxmls.deep_learning import mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_a = mlp.NumpyMLP(geometry, actvfunc)\n",
    "mlp_b = mlp.TheanoMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.97361002e-01,   9.95937933e-01,   9.92078585e-01,\n",
       "          9.94850280e-01,   9.98412851e-01,   9.88964620e-01,\n",
       "          9.98383107e-01,   9.97969292e-01,   9.98943300e-01,\n",
       "          9.98041211e-01,   9.99464753e-01,   9.99465586e-01,\n",
       "          9.96180055e-01,   9.98601670e-01,   9.98139010e-01,\n",
       "          9.98276461e-01,   9.97815365e-01,   9.99803597e-01,\n",
       "          9.98383107e-01,   9.97819214e-01,   9.98871978e-01,\n",
       "          9.99633575e-01,   9.96569806e-01,   9.97649499e-01,\n",
       "          9.98691966e-01,   9.95870304e-01,   9.97030871e-01,\n",
       "          9.99033700e-01,   9.98400779e-01,   9.98331404e-01,\n",
       "          9.98131659e-01,   9.98745672e-01,   9.99045619e-01,\n",
       "          9.99457658e-01,   9.97361002e-01,   9.97669707e-01,\n",
       "          9.95783678e-01,   9.98166247e-01,   9.98048320e-01,\n",
       "          9.98494150e-01,   9.97826168e-01,   9.97289112e-01,\n",
       "          9.96860902e-01,   9.95143452e-01,   9.97549919e-01,\n",
       "          9.97115796e-01,   9.98386999e-01,   9.99234555e-01,\n",
       "          9.99684445e-01,   9.96992160e-01,   9.98047541e-01,\n",
       "          9.98210926e-01,   9.96169665e-01,   9.98009408e-01,\n",
       "          9.98757478e-01,   9.99474826e-01,   9.96735853e-01,\n",
       "          9.98286203e-01,   9.98628345e-01,   9.96352627e-01,\n",
       "          9.99072777e-01,   9.96708421e-01,   9.97524210e-01,\n",
       "          9.98453208e-01,   9.97326190e-01,   9.97331726e-01,\n",
       "          9.99298654e-01,   9.94386657e-01,   9.99168785e-01,\n",
       "          9.97172812e-01,   9.96803754e-01,   9.97551160e-01,\n",
       "          9.94019525e-01,   9.97376261e-01,   9.98272839e-01,\n",
       "          9.98185509e-01,   9.98409556e-01,   9.97731399e-01,\n",
       "          9.99010581e-01,   9.97900783e-01,   9.95288934e-01,\n",
       "          9.98184055e-01,   9.97549534e-01,   9.97732374e-01,\n",
       "          9.88844012e-01,   9.98137548e-01,   9.97594979e-01,\n",
       "          9.93567848e-01,   9.97853761e-01,   9.96412328e-01,\n",
       "          9.98148520e-01,   9.99144686e-01,   9.98305702e-01,\n",
       "          9.98130787e-01,   9.99321520e-01,   9.99146551e-01,\n",
       "          9.96062920e-01,   9.99784951e-01,   9.99538801e-01,\n",
       "          9.98557529e-01,   9.99296190e-01,   9.96333418e-01,\n",
       "          9.97711611e-01,   9.93895479e-01,   9.98524049e-01,\n",
       "          9.98586644e-01,   9.98424988e-01,   9.98624159e-01,\n",
       "          9.99002815e-01,   9.98859325e-01,   9.99388841e-01,\n",
       "          9.99177335e-01,   9.97324598e-01,   9.95966940e-01,\n",
       "          9.97945948e-01,   9.98426717e-01,   9.98608655e-01,\n",
       "          9.95378433e-01,   9.98632516e-01,   9.95587885e-01,\n",
       "          9.99105611e-01,   9.97704110e-01,   9.99364566e-01,\n",
       "          9.99344371e-01,   9.98444059e-01,   9.99145888e-01,\n",
       "          9.98414753e-01,   9.99069476e-01,   9.96917094e-01,\n",
       "          9.96891398e-01,   9.99110493e-01,   9.96886890e-01,\n",
       "          9.97752896e-01,   9.98312714e-01,   9.97364679e-01,\n",
       "          9.97176450e-01,   9.98595973e-01,   9.97643866e-01,\n",
       "          9.98004875e-01,   9.88378602e-01,   9.98483896e-01,\n",
       "          9.99038573e-01,   9.98658838e-01,   9.97877774e-01,\n",
       "          9.97664967e-01,   9.99830453e-01,   9.98939287e-01,\n",
       "          9.96993569e-01,   9.98090955e-01,   9.97173417e-01,\n",
       "          9.96086905e-01,   9.99580608e-01,   9.97548779e-01,\n",
       "          9.97514317e-01,   9.98911485e-01,   9.97774976e-01,\n",
       "          9.97181854e-01,   9.98301246e-01,   9.98177292e-01,\n",
       "          9.98188109e-01,   9.99192816e-01,   9.97762160e-01,\n",
       "          9.97852083e-01,   9.98367750e-01,   9.97691321e-01,\n",
       "          9.98990302e-01,   9.98869684e-01,   9.98694456e-01,\n",
       "          9.98164057e-01,   9.96999182e-01,   9.98102225e-01,\n",
       "          9.97976000e-01,   9.97683034e-01,   9.99084621e-01,\n",
       "          9.97986266e-01,   9.94269809e-01,   9.99430694e-01,\n",
       "          9.98168072e-01,   9.98758962e-01,   9.98549200e-01,\n",
       "          9.94445098e-01,   9.98711502e-01,   9.94440997e-01,\n",
       "          9.98377049e-01,   9.97664012e-01,   9.84806154e-01,\n",
       "          9.98140526e-01,   9.98421219e-01,   9.98563490e-01,\n",
       "          9.97500833e-01,   9.96950145e-01,   9.98367537e-01,\n",
       "          9.92433079e-01,   9.99610063e-01,   9.98391240e-01,\n",
       "          9.99312119e-01,   9.99373274e-01,   9.99082093e-01,\n",
       "          9.99131431e-01,   9.99250750e-01,   9.98965440e-01,\n",
       "          9.98369038e-01,   9.99031517e-01,   9.98343930e-01,\n",
       "          9.99006497e-01,   9.99295110e-01,   9.99388386e-01,\n",
       "          9.99272996e-01,   9.98610751e-01,   9.98905062e-01,\n",
       "          9.96226467e-01,   9.94202564e-01,   9.99031517e-01,\n",
       "          9.98104759e-01,   9.98428820e-01,   9.99173722e-01,\n",
       "          9.98860901e-01,   9.98091450e-01,   9.97902141e-01,\n",
       "          9.98630450e-01,   9.96490347e-01,   9.99102307e-01,\n",
       "          9.98697399e-01,   9.98709068e-01,   9.97405340e-01,\n",
       "          9.54809772e-01,   9.98684221e-01,   9.98885518e-01,\n",
       "          9.99442646e-01,   9.99047616e-01,   9.98735441e-01,\n",
       "          9.98781487e-01,   9.98690520e-01,   9.98046060e-01,\n",
       "          9.98008652e-01,   9.99347659e-01,   9.96564111e-01,\n",
       "          9.99126795e-01,   9.98912316e-01,   9.99082646e-01,\n",
       "          9.98217320e-01,   9.97051793e-01,   9.98642883e-01,\n",
       "          9.98776383e-01,   9.98120772e-01,   9.95825111e-01,\n",
       "          9.98694058e-01,   9.93747142e-01,   9.98275077e-01,\n",
       "          9.95707653e-01,   9.98187023e-01,   9.97560690e-01,\n",
       "          9.97500856e-01,   9.98816494e-01,   9.98215946e-01,\n",
       "          9.95356421e-01,   9.94620252e-01,   9.95152520e-01,\n",
       "          9.98326369e-01,   9.98735382e-01,   9.98680059e-01,\n",
       "          9.97034036e-01,   9.96726007e-01,   9.98868713e-01,\n",
       "          9.98535542e-01,   9.98150650e-01,   9.99140733e-01,\n",
       "          9.99030346e-01,   9.99173659e-01,   9.99009453e-01,\n",
       "          9.97033827e-01,   9.98738827e-01,   9.99891884e-01,\n",
       "          9.99272996e-01,   9.98907215e-01,   9.99225356e-01,\n",
       "          9.98693378e-01,   9.97765582e-01,   9.94634148e-01,\n",
       "          9.97784380e-01,   9.99199791e-01,   9.97522880e-01,\n",
       "          9.97527050e-01,   9.98173729e-01,   9.96904940e-01,\n",
       "          9.98949847e-01,   9.99293990e-01,   9.95333697e-01,\n",
       "          9.98864351e-01,   9.97387716e-01,   9.97502686e-01,\n",
       "          9.97653672e-01,   9.99040419e-01,   9.97926730e-01,\n",
       "          9.98466025e-01,   9.90618498e-01,   9.85129429e-01,\n",
       "          9.95791665e-01,   9.97292496e-01,   9.99305270e-01,\n",
       "          9.98485343e-01,   9.98579249e-01,   9.99331729e-01,\n",
       "          9.99267352e-01,   9.98014571e-01,   9.98710201e-01,\n",
       "          9.95409289e-01,   9.98870057e-01,   9.96929218e-01,\n",
       "          9.99378344e-01,   9.95519071e-01,   9.97798836e-01,\n",
       "          9.98443259e-01,   9.96995901e-01,   9.98611417e-01,\n",
       "          9.98461035e-01,   9.99425505e-01,   9.98782333e-01,\n",
       "          9.96698817e-01,   9.99833520e-01,   9.97226517e-01,\n",
       "          9.99362638e-01,   9.97577440e-01,   9.98997500e-01,\n",
       "          9.98214770e-01,   9.97618911e-01,   9.98458842e-01,\n",
       "          9.99564785e-01,   9.97694380e-01,   9.95704124e-01,\n",
       "          9.95432581e-01,   9.98469589e-01,   9.98759700e-01,\n",
       "          9.97873562e-01,   9.98726950e-01,   9.98506186e-01,\n",
       "          9.96450592e-01,   9.98674971e-01,   9.96835760e-01,\n",
       "          9.99387711e-01,   9.97319762e-01,   9.97346805e-01,\n",
       "          9.99428911e-01,   9.99108997e-01,   9.97424707e-01,\n",
       "          9.97492650e-01,   9.98380544e-01,   9.97801664e-01,\n",
       "          9.99081590e-01,   9.98816543e-01,   9.98285934e-01,\n",
       "          9.94719267e-01,   9.98269911e-01,   9.97437437e-01,\n",
       "          9.98755503e-01,   9.93574937e-01,   9.98563446e-01,\n",
       "          9.99357989e-01,   9.97797690e-01,   9.98760356e-01,\n",
       "          9.98662356e-01,   9.96943142e-01,   9.97019657e-01,\n",
       "          9.98598283e-01,   9.91951568e-01,   9.99126795e-01,\n",
       "          9.97789638e-01,   9.98821600e-01,   9.98238395e-01,\n",
       "          9.98869243e-01,   9.97693359e-01,   9.99693454e-01,\n",
       "          9.98349903e-01,   9.99078023e-01,   9.99631900e-01,\n",
       "          9.98415410e-01,   9.99099807e-01,   9.99469798e-01,\n",
       "          9.98759522e-01,   9.99334363e-01,   9.98195665e-01,\n",
       "          9.98259674e-01,   9.98187801e-01,   9.99193019e-01,\n",
       "          9.93600513e-01,   9.99426617e-01,   9.94912225e-01,\n",
       "          9.94908045e-01,   9.98812971e-01,   9.98070309e-01,\n",
       "          9.97009606e-01,   9.98795199e-01,   9.98350326e-01,\n",
       "          9.97873207e-01,   9.98610117e-01,   9.99102323e-01,\n",
       "          9.97502295e-01,   9.97683398e-01,   9.98221429e-01,\n",
       "          9.99057282e-01],\n",
       "       [  2.63899834e-03,   4.06206710e-03,   7.92141509e-03,\n",
       "          5.14971986e-03,   1.58714875e-03,   1.10353799e-02,\n",
       "          1.61689301e-03,   2.03070850e-03,   1.05669971e-03,\n",
       "          1.95878879e-03,   5.35247479e-04,   5.34414352e-04,\n",
       "          3.81994533e-03,   1.39833040e-03,   1.86098983e-03,\n",
       "          1.72353882e-03,   2.18463470e-03,   1.96403063e-04,\n",
       "          1.61689301e-03,   2.18078557e-03,   1.12802175e-03,\n",
       "          3.66424826e-04,   3.43019444e-03,   2.35050068e-03,\n",
       "          1.30803396e-03,   4.12969573e-03,   2.96912859e-03,\n",
       "          9.66300315e-04,   1.59922145e-03,   1.66859584e-03,\n",
       "          1.86834129e-03,   1.25432836e-03,   9.54381477e-04,\n",
       "          5.42341597e-04,   2.63899834e-03,   2.33029306e-03,\n",
       "          4.21632153e-03,   1.83375268e-03,   1.95168014e-03,\n",
       "          1.50585008e-03,   2.17383199e-03,   2.71088837e-03,\n",
       "          3.13909822e-03,   4.85654753e-03,   2.45008110e-03,\n",
       "          2.88420433e-03,   1.61300132e-03,   7.65445376e-04,\n",
       "          3.15554858e-04,   3.00783981e-03,   1.95245924e-03,\n",
       "          1.78907409e-03,   3.83033456e-03,   1.99059227e-03,\n",
       "          1.24252175e-03,   5.25173945e-04,   3.26414662e-03,\n",
       "          1.71379719e-03,   1.37165462e-03,   3.64737306e-03,\n",
       "          9.27222920e-04,   3.29157918e-03,   2.47579018e-03,\n",
       "          1.54679239e-03,   2.67380960e-03,   2.66827399e-03,\n",
       "          7.01345541e-04,   5.61334317e-03,   8.31214650e-04,\n",
       "          2.82718830e-03,   3.19624569e-03,   2.44884044e-03,\n",
       "          5.98047497e-03,   2.62373852e-03,   1.72716150e-03,\n",
       "          1.81449125e-03,   1.59044394e-03,   2.26860054e-03,\n",
       "          9.89418808e-04,   2.09921659e-03,   4.71106598e-03,\n",
       "          1.81594506e-03,   2.45046594e-03,   2.26762578e-03,\n",
       "          1.11559881e-02,   1.86245213e-03,   2.40502070e-03,\n",
       "          6.43215163e-03,   2.14623901e-03,   3.58767178e-03,\n",
       "          1.85147978e-03,   8.55314036e-04,   1.69429826e-03,\n",
       "          1.86921269e-03,   6.78479819e-04,   8.53449373e-04,\n",
       "          3.93708012e-03,   2.15048614e-04,   4.61198997e-04,\n",
       "          1.44247096e-03,   7.03809891e-04,   3.66658239e-03,\n",
       "          2.28838899e-03,   6.10452129e-03,   1.47595108e-03,\n",
       "          1.41335601e-03,   1.57501203e-03,   1.37584063e-03,\n",
       "          9.97184573e-04,   1.14067499e-03,   6.11158898e-04,\n",
       "          8.22664817e-04,   2.67540172e-03,   4.03306049e-03,\n",
       "          2.05405161e-03,   1.57328313e-03,   1.39134454e-03,\n",
       "          4.62156746e-03,   1.36748440e-03,   4.41211542e-03,\n",
       "          8.94389013e-04,   2.29589009e-03,   6.35434081e-04,\n",
       "          6.55629129e-04,   1.55594083e-03,   8.54111846e-04,\n",
       "          1.58524718e-03,   9.30524135e-04,   3.08290622e-03,\n",
       "          3.10860240e-03,   8.89506918e-04,   3.11310965e-03,\n",
       "          2.24710353e-03,   1.68728624e-03,   2.63532124e-03,\n",
       "          2.82355015e-03,   1.40402705e-03,   2.35613372e-03,\n",
       "          1.99512466e-03,   1.16213978e-02,   1.51610419e-03,\n",
       "          9.61427217e-04,   1.34116151e-03,   2.12222585e-03,\n",
       "          2.33503296e-03,   1.69547158e-04,   1.06071253e-03,\n",
       "          3.00643098e-03,   1.90904471e-03,   2.82658280e-03,\n",
       "          3.91309492e-03,   4.19391547e-04,   2.45122115e-03,\n",
       "          2.48568316e-03,   1.08851491e-03,   2.22502428e-03,\n",
       "          2.81814616e-03,   1.69875436e-03,   1.82270768e-03,\n",
       "          1.81189054e-03,   8.07184055e-04,   2.23784004e-03,\n",
       "          2.14791731e-03,   1.63224975e-03,   2.30867874e-03,\n",
       "          1.00969809e-03,   1.13031622e-03,   1.30554410e-03,\n",
       "          1.83594300e-03,   3.00081775e-03,   1.89777467e-03,\n",
       "          2.02399951e-03,   2.31696610e-03,   9.15379278e-04,\n",
       "          2.01373431e-03,   5.73019068e-03,   5.69305684e-04,\n",
       "          1.83192799e-03,   1.24103751e-03,   1.45079953e-03,\n",
       "          5.55490238e-03,   1.28849848e-03,   5.55900317e-03,\n",
       "          1.62295122e-03,   2.33598754e-03,   1.51938461e-02,\n",
       "          1.85947449e-03,   1.57878129e-03,   1.43650975e-03,\n",
       "          2.49916692e-03,   3.04985477e-03,   1.63246338e-03,\n",
       "          7.56692054e-03,   3.89936567e-04,   1.60876023e-03,\n",
       "          6.87880744e-04,   6.26726283e-04,   9.17906951e-04,\n",
       "          8.68569481e-04,   7.49250237e-04,   1.03455992e-03,\n",
       "          1.63096222e-03,   9.68482821e-04,   1.65607012e-03,\n",
       "          9.93503149e-04,   7.04889659e-04,   6.11613985e-04,\n",
       "          7.27003814e-04,   1.38924852e-03,   1.09493772e-03,\n",
       "          3.77353317e-03,   5.79743583e-03,   9.68482821e-04,\n",
       "          1.89524097e-03,   1.57117962e-03,   8.26277899e-04,\n",
       "          1.13909918e-03,   1.90854957e-03,   2.09785889e-03,\n",
       "          1.36954972e-03,   3.50965290e-03,   8.97693315e-04,\n",
       "          1.30260126e-03,   1.29093158e-03,   2.59465979e-03,\n",
       "          4.51902278e-02,   1.31577919e-03,   1.11448164e-03,\n",
       "          5.57354375e-04,   9.52383899e-04,   1.26455890e-03,\n",
       "          1.21851260e-03,   1.30947983e-03,   1.95393953e-03,\n",
       "          1.99134804e-03,   6.52340770e-04,   3.43588943e-03,\n",
       "          8.73204642e-04,   1.08768367e-03,   9.17354358e-04,\n",
       "          1.78268033e-03,   2.94820728e-03,   1.35711651e-03,\n",
       "          1.22361701e-03,   1.87922799e-03,   4.17488934e-03,\n",
       "          1.30594223e-03,   6.25285773e-03,   1.72492254e-03,\n",
       "          4.29234698e-03,   1.81297721e-03,   2.43931019e-03,\n",
       "          2.49914441e-03,   1.18350615e-03,   1.78405361e-03,\n",
       "          4.64357905e-03,   5.37974847e-03,   4.84748000e-03,\n",
       "          1.67363129e-03,   1.26461776e-03,   1.31994077e-03,\n",
       "          2.96596415e-03,   3.27399260e-03,   1.13128742e-03,\n",
       "          1.46445810e-03,   1.84935003e-03,   8.59266680e-04,\n",
       "          9.69654457e-04,   8.26341095e-04,   9.90546516e-04,\n",
       "          2.96617272e-03,   1.26117293e-03,   1.08116441e-04,\n",
       "          7.27003814e-04,   1.09278512e-03,   7.74643510e-04,\n",
       "          1.30662152e-03,   2.23441820e-03,   5.36585192e-03,\n",
       "          2.21561993e-03,   8.00209438e-04,   2.47711987e-03,\n",
       "          2.47295033e-03,   1.82627062e-03,   3.09506003e-03,\n",
       "          1.05015339e-03,   7.06010008e-04,   4.66630338e-03,\n",
       "          1.13564924e-03,   2.61228389e-03,   2.49731362e-03,\n",
       "          2.34632814e-03,   9.59580910e-04,   2.07327026e-03,\n",
       "          1.53397480e-03,   9.38150187e-03,   1.48705713e-02,\n",
       "          4.20833468e-03,   2.70750435e-03,   6.94729605e-04,\n",
       "          1.51465713e-03,   1.42075140e-03,   6.68270514e-04,\n",
       "          7.32648353e-04,   1.98542894e-03,   1.28979916e-03,\n",
       "          4.59071070e-03,   1.12994253e-03,   3.07078225e-03,\n",
       "          6.21656444e-04,   4.48092874e-03,   2.20116420e-03,\n",
       "          1.55674078e-03,   3.00409855e-03,   1.38858294e-03,\n",
       "          1.53896492e-03,   5.74495182e-04,   1.21766657e-03,\n",
       "          3.30118262e-03,   1.66480218e-04,   2.77348324e-03,\n",
       "          6.37362060e-04,   2.42255971e-03,   1.00249965e-03,\n",
       "          1.78522957e-03,   2.38108912e-03,   1.54115827e-03,\n",
       "          4.35215442e-04,   2.30561991e-03,   4.29587626e-03,\n",
       "          4.56741932e-03,   1.53041075e-03,   1.24030049e-03,\n",
       "          2.12643794e-03,   1.27305045e-03,   1.49381382e-03,\n",
       "          3.54940844e-03,   1.32502877e-03,   3.16424029e-03,\n",
       "          6.12289232e-04,   2.68023765e-03,   2.65319492e-03,\n",
       "          5.71088902e-04,   8.91003346e-04,   2.57529266e-03,\n",
       "          2.50734970e-03,   1.61945585e-03,   2.19833633e-03,\n",
       "          9.18410246e-04,   1.18345699e-03,   1.71406578e-03,\n",
       "          5.28073342e-03,   1.73008863e-03,   2.56256319e-03,\n",
       "          1.24449663e-03,   6.42506282e-03,   1.43655415e-03,\n",
       "          6.42011357e-04,   2.20230998e-03,   1.23964373e-03,\n",
       "          1.33764358e-03,   3.05685811e-03,   2.98034306e-03,\n",
       "          1.40171732e-03,   8.04843209e-03,   8.73204642e-04,\n",
       "          2.21036189e-03,   1.17839971e-03,   1.76160549e-03,\n",
       "          1.13075715e-03,   2.30664093e-03,   3.06546041e-04,\n",
       "          1.65009697e-03,   9.21976992e-04,   3.68100151e-04,\n",
       "          1.58458968e-03,   9.00192745e-04,   5.30202429e-04,\n",
       "          1.24047779e-03,   6.65636944e-04,   1.80433542e-03,\n",
       "          1.74032584e-03,   1.81219851e-03,   8.06980633e-04,\n",
       "          6.39948669e-03,   5.73382729e-04,   5.08777525e-03,\n",
       "          5.09195502e-03,   1.18702905e-03,   1.92969100e-03,\n",
       "          2.99039403e-03,   1.20480098e-03,   1.64967428e-03,\n",
       "          2.12679347e-03,   1.38988278e-03,   8.97676979e-04,\n",
       "          2.49770464e-03,   2.31660231e-03,   1.77857071e-03,\n",
       "          9.42718330e-04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_a.forward(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.97361002e-01,   9.95937933e-01,   9.92078585e-01,\n",
       "          9.94850280e-01,   9.98412851e-01,   9.88964620e-01,\n",
       "          9.98383107e-01,   9.97969292e-01,   9.98943300e-01,\n",
       "          9.98041211e-01,   9.99464753e-01,   9.99465586e-01,\n",
       "          9.96180055e-01,   9.98601670e-01,   9.98139010e-01,\n",
       "          9.98276461e-01,   9.97815365e-01,   9.99803597e-01,\n",
       "          9.98383107e-01,   9.97819214e-01,   9.98871978e-01,\n",
       "          9.99633575e-01,   9.96569806e-01,   9.97649499e-01,\n",
       "          9.98691966e-01,   9.95870304e-01,   9.97030871e-01,\n",
       "          9.99033700e-01,   9.98400779e-01,   9.98331404e-01,\n",
       "          9.98131659e-01,   9.98745672e-01,   9.99045619e-01,\n",
       "          9.99457658e-01,   9.97361002e-01,   9.97669707e-01,\n",
       "          9.95783678e-01,   9.98166247e-01,   9.98048320e-01,\n",
       "          9.98494150e-01,   9.97826168e-01,   9.97289112e-01,\n",
       "          9.96860902e-01,   9.95143452e-01,   9.97549919e-01,\n",
       "          9.97115796e-01,   9.98386999e-01,   9.99234555e-01,\n",
       "          9.99684445e-01,   9.96992160e-01,   9.98047541e-01,\n",
       "          9.98210926e-01,   9.96169665e-01,   9.98009408e-01,\n",
       "          9.98757478e-01,   9.99474826e-01,   9.96735853e-01,\n",
       "          9.98286203e-01,   9.98628345e-01,   9.96352627e-01,\n",
       "          9.99072777e-01,   9.96708421e-01,   9.97524210e-01,\n",
       "          9.98453208e-01,   9.97326190e-01,   9.97331726e-01,\n",
       "          9.99298654e-01,   9.94386657e-01,   9.99168785e-01,\n",
       "          9.97172812e-01,   9.96803754e-01,   9.97551160e-01,\n",
       "          9.94019525e-01,   9.97376261e-01,   9.98272839e-01,\n",
       "          9.98185509e-01,   9.98409556e-01,   9.97731399e-01,\n",
       "          9.99010581e-01,   9.97900783e-01,   9.95288934e-01,\n",
       "          9.98184055e-01,   9.97549534e-01,   9.97732374e-01,\n",
       "          9.88844012e-01,   9.98137548e-01,   9.97594979e-01,\n",
       "          9.93567848e-01,   9.97853761e-01,   9.96412328e-01,\n",
       "          9.98148520e-01,   9.99144686e-01,   9.98305702e-01,\n",
       "          9.98130787e-01,   9.99321520e-01,   9.99146551e-01,\n",
       "          9.96062920e-01,   9.99784951e-01,   9.99538801e-01,\n",
       "          9.98557529e-01,   9.99296190e-01,   9.96333418e-01,\n",
       "          9.97711611e-01,   9.93895479e-01,   9.98524049e-01,\n",
       "          9.98586644e-01,   9.98424988e-01,   9.98624159e-01,\n",
       "          9.99002815e-01,   9.98859325e-01,   9.99388841e-01,\n",
       "          9.99177335e-01,   9.97324598e-01,   9.95966940e-01,\n",
       "          9.97945948e-01,   9.98426717e-01,   9.98608655e-01,\n",
       "          9.95378433e-01,   9.98632516e-01,   9.95587885e-01,\n",
       "          9.99105611e-01,   9.97704110e-01,   9.99364566e-01,\n",
       "          9.99344371e-01,   9.98444059e-01,   9.99145888e-01,\n",
       "          9.98414753e-01,   9.99069476e-01,   9.96917094e-01,\n",
       "          9.96891398e-01,   9.99110493e-01,   9.96886890e-01,\n",
       "          9.97752896e-01,   9.98312714e-01,   9.97364679e-01,\n",
       "          9.97176450e-01,   9.98595973e-01,   9.97643866e-01,\n",
       "          9.98004875e-01,   9.88378602e-01,   9.98483896e-01,\n",
       "          9.99038573e-01,   9.98658838e-01,   9.97877774e-01,\n",
       "          9.97664967e-01,   9.99830453e-01,   9.98939287e-01,\n",
       "          9.96993569e-01,   9.98090955e-01,   9.97173417e-01,\n",
       "          9.96086905e-01,   9.99580608e-01,   9.97548779e-01,\n",
       "          9.97514317e-01,   9.98911485e-01,   9.97774976e-01,\n",
       "          9.97181854e-01,   9.98301246e-01,   9.98177292e-01,\n",
       "          9.98188109e-01,   9.99192816e-01,   9.97762160e-01,\n",
       "          9.97852083e-01,   9.98367750e-01,   9.97691321e-01,\n",
       "          9.98990302e-01,   9.98869684e-01,   9.98694456e-01,\n",
       "          9.98164057e-01,   9.96999182e-01,   9.98102225e-01,\n",
       "          9.97976000e-01,   9.97683034e-01,   9.99084621e-01,\n",
       "          9.97986266e-01,   9.94269809e-01,   9.99430694e-01,\n",
       "          9.98168072e-01,   9.98758962e-01,   9.98549200e-01,\n",
       "          9.94445098e-01,   9.98711502e-01,   9.94440997e-01,\n",
       "          9.98377049e-01,   9.97664012e-01,   9.84806154e-01,\n",
       "          9.98140526e-01,   9.98421219e-01,   9.98563490e-01,\n",
       "          9.97500833e-01,   9.96950145e-01,   9.98367537e-01,\n",
       "          9.92433079e-01,   9.99610063e-01,   9.98391240e-01,\n",
       "          9.99312119e-01,   9.99373274e-01,   9.99082093e-01,\n",
       "          9.99131431e-01,   9.99250750e-01,   9.98965440e-01,\n",
       "          9.98369038e-01,   9.99031517e-01,   9.98343930e-01,\n",
       "          9.99006497e-01,   9.99295110e-01,   9.99388386e-01,\n",
       "          9.99272996e-01,   9.98610751e-01,   9.98905062e-01,\n",
       "          9.96226467e-01,   9.94202564e-01,   9.99031517e-01,\n",
       "          9.98104759e-01,   9.98428820e-01,   9.99173722e-01,\n",
       "          9.98860901e-01,   9.98091450e-01,   9.97902141e-01,\n",
       "          9.98630450e-01,   9.96490347e-01,   9.99102307e-01,\n",
       "          9.98697399e-01,   9.98709068e-01,   9.97405340e-01,\n",
       "          9.54809772e-01,   9.98684221e-01,   9.98885518e-01,\n",
       "          9.99442646e-01,   9.99047616e-01,   9.98735441e-01,\n",
       "          9.98781487e-01,   9.98690520e-01,   9.98046060e-01,\n",
       "          9.98008652e-01,   9.99347659e-01,   9.96564111e-01,\n",
       "          9.99126795e-01,   9.98912316e-01,   9.99082646e-01,\n",
       "          9.98217320e-01,   9.97051793e-01,   9.98642883e-01,\n",
       "          9.98776383e-01,   9.98120772e-01,   9.95825111e-01,\n",
       "          9.98694058e-01,   9.93747142e-01,   9.98275077e-01,\n",
       "          9.95707653e-01,   9.98187023e-01,   9.97560690e-01,\n",
       "          9.97500856e-01,   9.98816494e-01,   9.98215946e-01,\n",
       "          9.95356421e-01,   9.94620252e-01,   9.95152520e-01,\n",
       "          9.98326369e-01,   9.98735382e-01,   9.98680059e-01,\n",
       "          9.97034036e-01,   9.96726007e-01,   9.98868713e-01,\n",
       "          9.98535542e-01,   9.98150650e-01,   9.99140733e-01,\n",
       "          9.99030346e-01,   9.99173659e-01,   9.99009453e-01,\n",
       "          9.97033827e-01,   9.98738827e-01,   9.99891884e-01,\n",
       "          9.99272996e-01,   9.98907215e-01,   9.99225356e-01,\n",
       "          9.98693378e-01,   9.97765582e-01,   9.94634148e-01,\n",
       "          9.97784380e-01,   9.99199791e-01,   9.97522880e-01,\n",
       "          9.97527050e-01,   9.98173729e-01,   9.96904940e-01,\n",
       "          9.98949847e-01,   9.99293990e-01,   9.95333697e-01,\n",
       "          9.98864351e-01,   9.97387716e-01,   9.97502686e-01,\n",
       "          9.97653672e-01,   9.99040419e-01,   9.97926730e-01,\n",
       "          9.98466025e-01,   9.90618498e-01,   9.85129429e-01,\n",
       "          9.95791665e-01,   9.97292496e-01,   9.99305270e-01,\n",
       "          9.98485343e-01,   9.98579249e-01,   9.99331729e-01,\n",
       "          9.99267352e-01,   9.98014571e-01,   9.98710201e-01,\n",
       "          9.95409289e-01,   9.98870057e-01,   9.96929218e-01,\n",
       "          9.99378344e-01,   9.95519071e-01,   9.97798836e-01,\n",
       "          9.98443259e-01,   9.96995901e-01,   9.98611417e-01,\n",
       "          9.98461035e-01,   9.99425505e-01,   9.98782333e-01,\n",
       "          9.96698817e-01,   9.99833520e-01,   9.97226517e-01,\n",
       "          9.99362638e-01,   9.97577440e-01,   9.98997500e-01,\n",
       "          9.98214770e-01,   9.97618911e-01,   9.98458842e-01,\n",
       "          9.99564785e-01,   9.97694380e-01,   9.95704124e-01,\n",
       "          9.95432581e-01,   9.98469589e-01,   9.98759700e-01,\n",
       "          9.97873562e-01,   9.98726950e-01,   9.98506186e-01,\n",
       "          9.96450592e-01,   9.98674971e-01,   9.96835760e-01,\n",
       "          9.99387711e-01,   9.97319762e-01,   9.97346805e-01,\n",
       "          9.99428911e-01,   9.99108997e-01,   9.97424707e-01,\n",
       "          9.97492650e-01,   9.98380544e-01,   9.97801664e-01,\n",
       "          9.99081590e-01,   9.98816543e-01,   9.98285934e-01,\n",
       "          9.94719267e-01,   9.98269911e-01,   9.97437437e-01,\n",
       "          9.98755503e-01,   9.93574937e-01,   9.98563446e-01,\n",
       "          9.99357989e-01,   9.97797690e-01,   9.98760356e-01,\n",
       "          9.98662356e-01,   9.96943142e-01,   9.97019657e-01,\n",
       "          9.98598283e-01,   9.91951568e-01,   9.99126795e-01,\n",
       "          9.97789638e-01,   9.98821600e-01,   9.98238395e-01,\n",
       "          9.98869243e-01,   9.97693359e-01,   9.99693454e-01,\n",
       "          9.98349903e-01,   9.99078023e-01,   9.99631900e-01,\n",
       "          9.98415410e-01,   9.99099807e-01,   9.99469798e-01,\n",
       "          9.98759522e-01,   9.99334363e-01,   9.98195665e-01,\n",
       "          9.98259674e-01,   9.98187801e-01,   9.99193019e-01,\n",
       "          9.93600513e-01,   9.99426617e-01,   9.94912225e-01,\n",
       "          9.94908045e-01,   9.98812971e-01,   9.98070309e-01,\n",
       "          9.97009606e-01,   9.98795199e-01,   9.98350326e-01,\n",
       "          9.97873207e-01,   9.98610117e-01,   9.99102323e-01,\n",
       "          9.97502295e-01,   9.97683398e-01,   9.98221429e-01,\n",
       "          9.99057282e-01],\n",
       "       [  2.63899834e-03,   4.06206710e-03,   7.92141509e-03,\n",
       "          5.14971986e-03,   1.58714875e-03,   1.10353799e-02,\n",
       "          1.61689301e-03,   2.03070850e-03,   1.05669971e-03,\n",
       "          1.95878879e-03,   5.35247479e-04,   5.34414352e-04,\n",
       "          3.81994533e-03,   1.39833040e-03,   1.86098983e-03,\n",
       "          1.72353882e-03,   2.18463470e-03,   1.96403063e-04,\n",
       "          1.61689301e-03,   2.18078557e-03,   1.12802175e-03,\n",
       "          3.66424826e-04,   3.43019444e-03,   2.35050068e-03,\n",
       "          1.30803396e-03,   4.12969573e-03,   2.96912859e-03,\n",
       "          9.66300315e-04,   1.59922145e-03,   1.66859584e-03,\n",
       "          1.86834129e-03,   1.25432836e-03,   9.54381477e-04,\n",
       "          5.42341597e-04,   2.63899834e-03,   2.33029306e-03,\n",
       "          4.21632153e-03,   1.83375268e-03,   1.95168014e-03,\n",
       "          1.50585008e-03,   2.17383199e-03,   2.71088837e-03,\n",
       "          3.13909822e-03,   4.85654753e-03,   2.45008110e-03,\n",
       "          2.88420433e-03,   1.61300132e-03,   7.65445376e-04,\n",
       "          3.15554858e-04,   3.00783981e-03,   1.95245924e-03,\n",
       "          1.78907409e-03,   3.83033456e-03,   1.99059227e-03,\n",
       "          1.24252175e-03,   5.25173945e-04,   3.26414662e-03,\n",
       "          1.71379719e-03,   1.37165462e-03,   3.64737306e-03,\n",
       "          9.27222920e-04,   3.29157918e-03,   2.47579018e-03,\n",
       "          1.54679239e-03,   2.67380960e-03,   2.66827399e-03,\n",
       "          7.01345541e-04,   5.61334317e-03,   8.31214650e-04,\n",
       "          2.82718830e-03,   3.19624569e-03,   2.44884044e-03,\n",
       "          5.98047497e-03,   2.62373852e-03,   1.72716150e-03,\n",
       "          1.81449125e-03,   1.59044394e-03,   2.26860054e-03,\n",
       "          9.89418808e-04,   2.09921659e-03,   4.71106598e-03,\n",
       "          1.81594506e-03,   2.45046594e-03,   2.26762578e-03,\n",
       "          1.11559881e-02,   1.86245213e-03,   2.40502070e-03,\n",
       "          6.43215163e-03,   2.14623901e-03,   3.58767178e-03,\n",
       "          1.85147978e-03,   8.55314036e-04,   1.69429826e-03,\n",
       "          1.86921269e-03,   6.78479819e-04,   8.53449373e-04,\n",
       "          3.93708012e-03,   2.15048614e-04,   4.61198997e-04,\n",
       "          1.44247096e-03,   7.03809891e-04,   3.66658239e-03,\n",
       "          2.28838899e-03,   6.10452129e-03,   1.47595108e-03,\n",
       "          1.41335601e-03,   1.57501203e-03,   1.37584063e-03,\n",
       "          9.97184573e-04,   1.14067499e-03,   6.11158898e-04,\n",
       "          8.22664817e-04,   2.67540172e-03,   4.03306049e-03,\n",
       "          2.05405161e-03,   1.57328313e-03,   1.39134454e-03,\n",
       "          4.62156746e-03,   1.36748440e-03,   4.41211542e-03,\n",
       "          8.94389013e-04,   2.29589009e-03,   6.35434081e-04,\n",
       "          6.55629129e-04,   1.55594083e-03,   8.54111846e-04,\n",
       "          1.58524718e-03,   9.30524135e-04,   3.08290622e-03,\n",
       "          3.10860240e-03,   8.89506918e-04,   3.11310965e-03,\n",
       "          2.24710353e-03,   1.68728624e-03,   2.63532124e-03,\n",
       "          2.82355015e-03,   1.40402705e-03,   2.35613372e-03,\n",
       "          1.99512466e-03,   1.16213978e-02,   1.51610419e-03,\n",
       "          9.61427217e-04,   1.34116151e-03,   2.12222585e-03,\n",
       "          2.33503296e-03,   1.69547158e-04,   1.06071253e-03,\n",
       "          3.00643098e-03,   1.90904471e-03,   2.82658280e-03,\n",
       "          3.91309492e-03,   4.19391547e-04,   2.45122115e-03,\n",
       "          2.48568316e-03,   1.08851491e-03,   2.22502428e-03,\n",
       "          2.81814616e-03,   1.69875436e-03,   1.82270768e-03,\n",
       "          1.81189054e-03,   8.07184055e-04,   2.23784004e-03,\n",
       "          2.14791731e-03,   1.63224975e-03,   2.30867874e-03,\n",
       "          1.00969809e-03,   1.13031622e-03,   1.30554410e-03,\n",
       "          1.83594300e-03,   3.00081775e-03,   1.89777467e-03,\n",
       "          2.02399951e-03,   2.31696610e-03,   9.15379278e-04,\n",
       "          2.01373431e-03,   5.73019068e-03,   5.69305684e-04,\n",
       "          1.83192799e-03,   1.24103751e-03,   1.45079953e-03,\n",
       "          5.55490238e-03,   1.28849848e-03,   5.55900317e-03,\n",
       "          1.62295122e-03,   2.33598754e-03,   1.51938461e-02,\n",
       "          1.85947449e-03,   1.57878129e-03,   1.43650975e-03,\n",
       "          2.49916692e-03,   3.04985477e-03,   1.63246338e-03,\n",
       "          7.56692054e-03,   3.89936567e-04,   1.60876023e-03,\n",
       "          6.87880744e-04,   6.26726283e-04,   9.17906951e-04,\n",
       "          8.68569481e-04,   7.49250237e-04,   1.03455992e-03,\n",
       "          1.63096222e-03,   9.68482821e-04,   1.65607012e-03,\n",
       "          9.93503149e-04,   7.04889659e-04,   6.11613985e-04,\n",
       "          7.27003814e-04,   1.38924852e-03,   1.09493772e-03,\n",
       "          3.77353317e-03,   5.79743583e-03,   9.68482821e-04,\n",
       "          1.89524097e-03,   1.57117962e-03,   8.26277899e-04,\n",
       "          1.13909918e-03,   1.90854957e-03,   2.09785889e-03,\n",
       "          1.36954972e-03,   3.50965290e-03,   8.97693315e-04,\n",
       "          1.30260126e-03,   1.29093158e-03,   2.59465979e-03,\n",
       "          4.51902278e-02,   1.31577919e-03,   1.11448164e-03,\n",
       "          5.57354375e-04,   9.52383899e-04,   1.26455890e-03,\n",
       "          1.21851260e-03,   1.30947983e-03,   1.95393953e-03,\n",
       "          1.99134804e-03,   6.52340770e-04,   3.43588943e-03,\n",
       "          8.73204642e-04,   1.08768367e-03,   9.17354358e-04,\n",
       "          1.78268033e-03,   2.94820728e-03,   1.35711651e-03,\n",
       "          1.22361701e-03,   1.87922799e-03,   4.17488934e-03,\n",
       "          1.30594223e-03,   6.25285773e-03,   1.72492254e-03,\n",
       "          4.29234698e-03,   1.81297721e-03,   2.43931019e-03,\n",
       "          2.49914441e-03,   1.18350615e-03,   1.78405361e-03,\n",
       "          4.64357905e-03,   5.37974847e-03,   4.84748000e-03,\n",
       "          1.67363129e-03,   1.26461776e-03,   1.31994077e-03,\n",
       "          2.96596415e-03,   3.27399260e-03,   1.13128742e-03,\n",
       "          1.46445810e-03,   1.84935003e-03,   8.59266680e-04,\n",
       "          9.69654457e-04,   8.26341095e-04,   9.90546516e-04,\n",
       "          2.96617272e-03,   1.26117293e-03,   1.08116441e-04,\n",
       "          7.27003814e-04,   1.09278512e-03,   7.74643510e-04,\n",
       "          1.30662152e-03,   2.23441820e-03,   5.36585192e-03,\n",
       "          2.21561993e-03,   8.00209438e-04,   2.47711987e-03,\n",
       "          2.47295033e-03,   1.82627062e-03,   3.09506003e-03,\n",
       "          1.05015339e-03,   7.06010008e-04,   4.66630338e-03,\n",
       "          1.13564924e-03,   2.61228389e-03,   2.49731362e-03,\n",
       "          2.34632814e-03,   9.59580910e-04,   2.07327026e-03,\n",
       "          1.53397480e-03,   9.38150187e-03,   1.48705713e-02,\n",
       "          4.20833468e-03,   2.70750435e-03,   6.94729605e-04,\n",
       "          1.51465713e-03,   1.42075140e-03,   6.68270514e-04,\n",
       "          7.32648353e-04,   1.98542894e-03,   1.28979916e-03,\n",
       "          4.59071070e-03,   1.12994253e-03,   3.07078225e-03,\n",
       "          6.21656444e-04,   4.48092874e-03,   2.20116420e-03,\n",
       "          1.55674078e-03,   3.00409855e-03,   1.38858294e-03,\n",
       "          1.53896492e-03,   5.74495182e-04,   1.21766657e-03,\n",
       "          3.30118262e-03,   1.66480218e-04,   2.77348324e-03,\n",
       "          6.37362060e-04,   2.42255971e-03,   1.00249965e-03,\n",
       "          1.78522957e-03,   2.38108912e-03,   1.54115827e-03,\n",
       "          4.35215442e-04,   2.30561991e-03,   4.29587626e-03,\n",
       "          4.56741932e-03,   1.53041075e-03,   1.24030049e-03,\n",
       "          2.12643794e-03,   1.27305045e-03,   1.49381382e-03,\n",
       "          3.54940844e-03,   1.32502877e-03,   3.16424029e-03,\n",
       "          6.12289232e-04,   2.68023765e-03,   2.65319492e-03,\n",
       "          5.71088902e-04,   8.91003346e-04,   2.57529266e-03,\n",
       "          2.50734970e-03,   1.61945585e-03,   2.19833633e-03,\n",
       "          9.18410246e-04,   1.18345699e-03,   1.71406578e-03,\n",
       "          5.28073342e-03,   1.73008863e-03,   2.56256319e-03,\n",
       "          1.24449663e-03,   6.42506282e-03,   1.43655415e-03,\n",
       "          6.42011357e-04,   2.20230998e-03,   1.23964373e-03,\n",
       "          1.33764358e-03,   3.05685811e-03,   2.98034306e-03,\n",
       "          1.40171732e-03,   8.04843209e-03,   8.73204642e-04,\n",
       "          2.21036189e-03,   1.17839971e-03,   1.76160549e-03,\n",
       "          1.13075715e-03,   2.30664093e-03,   3.06546041e-04,\n",
       "          1.65009697e-03,   9.21976992e-04,   3.68100151e-04,\n",
       "          1.58458968e-03,   9.00192745e-04,   5.30202429e-04,\n",
       "          1.24047779e-03,   6.65636944e-04,   1.80433542e-03,\n",
       "          1.74032584e-03,   1.81219851e-03,   8.06980633e-04,\n",
       "          6.39948669e-03,   5.73382729e-04,   5.08777525e-03,\n",
       "          5.09195502e-03,   1.18702905e-03,   1.92969100e-03,\n",
       "          2.99039403e-03,   1.20480098e-03,   1.64967428e-03,\n",
       "          2.12679347e-03,   1.38988278e-03,   8.97676979e-04,\n",
       "          2.49770464e-03,   2.31660231e-03,   1.77857071e-03,\n",
       "          9.42718330e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mlp_b.forward(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.allclose(mlp_a.forward(test_x), mlp_b.forward(test_x)), \"ERROR: Numpy and Theano forward passes differ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Exercise 5.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic differentiation\n",
    "In the previous section we compiled the forward pass of a MLP. In this section\n",
    "we will do the same with the cost used for training. We will also derive the\n",
    "gradients although this will be trivial once we have the cost function compiled.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4\n",
    "\n",
    "We first see an example that does not use any of the code in TheanoMLP but\n",
    "rather continues from what you wrote in exercise 6.3. In this exercise you\n",
    "completed a sigmoid layer with Theano. To get some values for the weights we\n",
    "used the first layer of the network you trained in 6.2. now we are going to use\n",
    "the second layer as well. This is thus assuming that your network in 6.2 has\n",
    "only two layers e.g. the recommended geometry (I, 20, 2). Make sure this is the\n",
    "case before starting this exercise.  \n",
    "\n",
    "For the sake of clarity, lets write here the part of Ex. 5.2 that we had completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the values from our MLP from Ex 6.2\n",
    "W1, b1   = mlp_a.params[:2]     # Weights and bias of fist layer \n",
    "\n",
    "# First layer symbolic variables\n",
    "_x  = T.matrix('x')\n",
    "_W1 = theano.shared(value=W1, name='W1', borrow=True) \n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) \n",
    "# First layer symbolic expressions\n",
    "_z1       = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1 = T.nnet.sigmoid(_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# print len(mlp_a.params[2:4])\n",
    "W2, b2 = mlp_a.params[2:4] # Weights and bias of second (and last!) layer\n",
    "\n",
    "# Second layer symbolic variables\n",
    "_W2 = theano.shared(value=W2, name='W2', borrow=True)\n",
    "_b2 = theano.shared(value=b2, name='b2', borrow=True, broadcastable=(False, True))\n",
    "\n",
    "# Second layer symbolic expressions\n",
    "_z2 = T.dot(_W2, _tilde_z1) + _b2\n",
    "_tilde_z2 = T.nnet.softmax(_z2.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_y = T.ivector('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_F = -T.mean(T.log(_tilde_z2[_y, T.arange(_y.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_nabla_F = T.grad(_F, _W1)\n",
    "nabla_F = theano.function([_x, _y], _nabla_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_c = mlp.TheanoMLP(geometry, actvfunc)\n",
    "\n",
    "_x = T.matrix('x')\n",
    "_y = T.ivector('y')\n",
    "_F = mlp_c._cost(_x, _y)\n",
    "\n",
    "# SGD update rule\n",
    "updates = [(par, par - lrate*T.grad(_F, par)) for par in mlp_c.params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_up = theano.function([_x, _y], _F, updates=updates)\n",
    "\n",
    "n_batch = int(np.ceil(float(train_x.shape[1])/bsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Model\n",
    "geometry = [train_x.shape[0], 20, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5L,)\n",
      "(2L, 5L)\n",
      "[[ 0.98467983  0.8838789   0.92263496  0.97481781  0.91888146]\n",
      " [ 0.01532017  0.1161211   0.07736504  0.02518219  0.08111854]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0b81dc219a1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minit_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\nNumpy version took %2.2f sec\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minit_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\sgd.pyc\u001b[0m in \u001b[0;36mSGD_train\u001b[1;34m(model, n_iter, bsize, lrate, train_set, batch_up, n_batch, devel_set, model_dbg)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mbatch_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[1;31m# INFO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\rBatch %d/%d (%d%%) \"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\sgd.pyc\u001b[0m in \u001b[0;36mbatch_up\u001b[1;34m(batch_x, batch_y)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# Get gradients for each layer and this batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Get gradients for each layer and this batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mnabla_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;31m# Update each parameter with SGD rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\mlp.pyc\u001b[0m in \u001b[0;36mgrads\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                     \u001b[0mnon_linear_error\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex2onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mactivations_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[0mprod_nle_activation\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_linear_error\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations_n_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\VANIA\\1.3 - Research\\Publishing, presentations and schools\\(S) LxMLS\\2016\\Labs\\lxmls-toolkit\\lxmls\\deep_learning\\mlp.pyc\u001b[0m in \u001b[0;36mindex2onehot\u001b[1;34m(index, N)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mOutput\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0monehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Numpy MLP\n",
    "mlp_a = mlp.NumpyMLP(geometry, actvfunc)\n",
    "init_t = time.clock()\n",
    "\n",
    "sgd.SGD_train(mlp_a, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "print \"\\nNumpy version took %2.2f sec\" % (time.clock() - init_t)\n",
    "\n",
    "acc_train = sgd.class_acc(mlp_a.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_a.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 320/320 (100%)   Epoch  1/ 5 in 5.26 seg\n",
      "Batch 320/320 (100%)   Epoch  2/ 5 in 5.21 seg\n",
      "Batch 320/320 (100%)   Epoch  3/ 5 in 5.32 seg\n",
      "Batch 320/320 (100%)   Epoch  4/ 5 in 5.39 seg\n",
      "Batch 320/320 (100%)   Epoch  5/ 5 in 5.19 seg\n",
      " \n",
      "\n",
      "Compiled gradient version took 26.37 sec\n",
      "Amazon Sentiment Accuracy train: 0.953750 test: 0.785000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Theano grads\n",
    "mlp_b = mlp.TheanoMLP(geometry, actvfunc)\n",
    "init_t = time.clock()\n",
    "\n",
    "sgd.SGD_train(mlp_b, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "print \"\\nCompiled gradient version took %2.2f sec\" % (time.clock() - init_t)\n",
    "\n",
    "acc_train = sgd.class_acc(mlp_b.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_b.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 320/320 (100%)   Epoch  1/ 5 in 11.07 seg\n",
      "Batch 320/320 (100%)   Epoch  2/ 5 in 10.58 seg\n",
      "Batch 320/320 (100%)   Epoch  3/ 5 in 10.60 seg\n",
      "Batch 320/320 (100%)   Epoch  4/ 5 in 10.58 seg\n",
      "Batch 320/320 (100%)   Epoch  5/ 5 in 10.97 seg\n",
      " \n",
      "\n",
      "Theano compiled batch update version took 53.81\n",
      "Amazon Sentiment Accuracy train: 0.948125 test: 0.760000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Theano batch update\n",
    "init_t = time.clock()\n",
    "\n",
    "sgd.SGD_train(mlp_c, n_iter, batch_up=batch_up, n_batch=n_batch, bsize=bsize,\n",
    "train_set=(train_x, train_y))\n",
    "print \"\\nTheano compiled batch update version took %2.2f\" % (time.clock() - init_t)\n",
    "\n",
    "acc_train = sgd.class_acc(mlp_c.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_c.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "Y = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_1_shape = (2,3)\n",
    "W_2_shape = (3,2)\n",
    "\n",
    "W_1 = np.random.normal(loc=0,scale=0.1, size=W_1_shape)\n",
    "b_1 = np.zeros(3)\n",
    "\n",
    "W_2 = np.random.normal(loc=0,scale=0.1, size=W_2_shape)\n",
    "b_2 = np.zeros(2)\n",
    "\n",
    "weights = [W_1, W_2]\n",
    "biases = [b_1,b_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class activation_layer(object):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    z = np.array(z)\n",
    "    return z*(z>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relu([5,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax (z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax(np.array([5,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activations = [relu, softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CrossEntropy(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"\\nTraining cost: CrossEntropy \")\n",
    "\n",
    "    def compute_cost(self, pred, target):\n",
    "        \"\"\"\n",
    "        Return the cost associated for \"Xpred\" and desired output \"Xtarget\".\n",
    "        Note that the function is vectorized and\n",
    "           - rows in pred are training examples. \n",
    "           - rows in target are output predictions\n",
    "\n",
    "        \"\"\"\n",
    "        return np.mean((target - pred)**2)\n",
    "\n",
    "    def delta(self, preactivation, activation, target):\n",
    "        \"\"\"\n",
    "        Return the error delta from the output layer.\n",
    "        \"\"\"\n",
    "        # nablaC_activation = (activation-target)\n",
    "        delta = 0.5 * (activation - target) #* sigmoid_prime(preactivation)\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_propagate(x, weights, biases, activations):\n",
    "    \"\"\"\n",
    "    Computes the activations of the first hidden layer and output layer.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    z_1 = np.dot(x,W1) + b_1\n",
    "    z_1_tilde = act1(z1)\n",
    "    activations.append(z_1_tilde )\n",
    "    \n",
    "    z_2 = np.dot(z_1_tilde, W2) + b_2\n",
    "    z_2_tilde =  act2(z_2)\n",
    "    activations.append(z_2_tilde)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_grad(activations, target, cost):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    output = activations[-1]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
